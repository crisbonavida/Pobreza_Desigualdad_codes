--- 
title: "Apéndices del Libro Pobreza y Desigualdad en R"
#date: "`r Sys.Date()`"
author:
  - Bonavida, Cristian - CEDLAS[^1]
  - Laguinge, Luis - CEDLAS[^2]
  - Varvasino, Joaquín - CEDLAS[^3]
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
description: " Nos propusimos trasncribir al lenguaje R y Python los apéndices del libro Pobreza y Desigualdad en América Latina de Gasparini, Cicowiez y Sosa Escudero que originalmente fueron escritos para Stata."
link-citations: yes
github-repo: "crisbonavida/Pobreza_Desigualdad_codes"
cover-image: "portada_libro_cover.png"
always_allow_html: true

---

# Bienvenidos

## Sobre este Proyecto: {-}

Nos propusimos trasncribir al lenguaje R y Python los apéndices del libro "Pobreza y Desigualdad en América Latina" de Gasparini, Cicowiez y Sosa Escudero que originalmente fueron escritos para Stata y que permitían replicar los datos e información presentados por los autores en el texto. Cada capítulo consta de un apéndice con códigos que permiten llevar a la práctica los conceptos desarrollados. Aquí **los traducimos a nuevos lenguajes y los presentamos en un formato amigable para permitir que un público más amplio y de diversas disciplinas pueda aprovecharlos**. 

El objetivo de este mini-proyecto no es otro que poner a disposición de un público más amplio estas herramientas útiles y mantener actualizado un material único, que ayuda a adentrarse y trabajar sobre la temática de pobreza y desigualdad. Es por eso que este material es de carácter complementario a las explicaciones y detalles conceptuales que se presentan en el libro de texto y los apéndices.



## El Libro {-}

Si esta es la primera vez que te encontrás con este libro, antes de empezar con los códigos, dejanos presentartelo. 

El propósito del libro es ayudar al lector interesado en América Latina a que recorra el arduo camino entre los datos y el reporte de resultados rigurosos que puedan contribuir al debate sobre la pobreza y la desigualdad en la región. El volumen busca poner al alcance del lector un conjunto de instrumentos que lo motiven hacia la investigación empírica, y que le permitan producir resultados de la manera más rigurosa posible, para así contribuir a los objetivos últimos de explicar y cambiar la realidad social de la región. Las discusiones conceptuales y analíticas son ilustradas con ejemplos concretos construidos con datos de los países de la región.

```{r, echo=FALSE, out.width='60%', fig.asp=.75, fig.align='center' }
knitr::include_graphics("portada_libro.png")
```

Una enorme ventaja de este trabajo es que se encuentra disponible para todos, ya que se lo puede descargar gratuitamente desde el la [página del libro](https://www.cedlas.econo.unlp.edu.ar/wp/publicaciones/libros/pobreza-y-desigualdad-en-america-latina/). Te invitamos a que puedas recorrerlo, leerlo y dedicarle varios minutos antes de adentrarte en las próximas secciones.


## Cómo aprovechar este material {-}

Un punto importante es que estos códigos que te presentamos están atados a los contenidos y explicaciones que se desarrollan a lo largo de los capítulos del libro. Por eso notarás que los códigos no son autocontenidos al 100%, es decir si bien refuerzan las ideas principales detrás de cada indicador, de cada gráfico o cada estimación, y están acompañados de instrucciones generales, no cubren en profundidad los conceptos teóricos e incluso prácticos detrás de su uso. Por es que decimos que **este material es de carácter complementario a las explicaciones y detalles conceptuales que se presentan en el libro de texto y los apéndices**. Para aprovecharlas al máximo te recomendamos tener abierta junto con la pestaña de R o Python, el pdf del libro de texto para ir siguiendo capitulo a capitulo los contenidos. De esta forma no solo será posible replicar las estimaciones sino también comprenderlas y saber entender qué nos dicen y qué no nos dicen.


## Qué es y qué no es {-}

Es importante fijar las expectativas desde el principio. En estos códigos vas a encontrar mucha información y muchos temas a cubrir, mientras que otros nos quedarán para una próxima entrega. En este sentido, este material **no busca ser una guía exhaustiva a modo de tutorial para iniciarse en el lenguaje de programación**. El propósito será seguir los códigos presentados en los apéndices del libro con la misma estructura temática y con el objetivo de facilitar nuevas herramientas al usuario en base a los que ya fue escrito. Por lo que si bien el material es útil para entrenarse en este lenguaje,  es más bien un proyecto que permite expandir sobre conocimientos y nociones previas. Es por eso que  se recomienda que el lector ya esté algo familiarizado con la sintaxis y el programa. Si no este no es tu caso,  no te preocupes!, te vamos a dejar a continuación  varias referencias para que puedas entrar en calor y ponerte a punto antes de empezar.

* Tutoriales para inciarse en R

  + https://martinmontane.github.io/CienciaDeDatosBook/index.html
  
  + https://diegokoz.github.io/Curso_R_EPH_clases/


* Para hacer el camino desde Stata

  +	https://www.matthieugomez.com/statar/index.html


## Qué necesitamos antes de arrancar {-}

Para poder seguir los códigos que te presentamos vas a necesitar descargarte (o al menos tener acceso) a las bases de datos sobre las que iremos trabajando. Estas bases de datos son las encuestas que desarrolla cada país y que el CEDLAS sistematiza para ofrecerlas en un formato usable para los investigadores. En el siguiente [link](https://www.cedlas.econo.unlp.edu.ar/wp/publicaciones/libros/pobreza-y-desigualdad-en-america-latina/#1505501369949-15c93bca-b4f8) encontrarás el repositorio.

<p>&nbsp;</p>


[^1]: __Cristian__ estudió la Licenciatura en Economía (UNNE) y la Maestría en Economía (UNLP). Actualmente colabora como investigador en CEDLAS. Sus intereses se centran en temas de desigualdad, cambio tecnológico y movilidad social. Para entrar en contacto podes escribirle a cristianbonavida@gmail.com. Para conocer más de Cristian, sus proyectos y publicaciones podes visitar su perfil en [twitter](https://twitter.com/crisbonavida) o [linkedin](https://www.linkedin.com/in/cristian-bonavida-966978160/)

[^2]: __Luis__ estudió la Licenciatura en Economía (UNC) y se graduó de la Maestría en Economía (UNLP). Actualmente trabaja como investigador en CEDLAS en temas de desigualdad, mercado laboral y políticas fiscales. Para entrar en contacto podes escribirle a luislaguinge4@gmail.com. Para conocer más de Luis, sus proyectos y publicaciones podes visitar su perfil [twitter](https://twitter.com/luislaguinge) o [linkedin](https://www.linkedin.com/in/luislaguinge/)

[^3]: __Joaquín__ estudió la Licenciatura y la Maestría en Economía (UNLP). Actualmente trabaja como investigador en CEDLAS en temas de desigualdad y mercado laboral. Para entrar en contacto podes escribirle a joaquinvarvasino@hotmail.com. Para conocer más de Joaquín, sus proyectos y publicaciones podes visitar su perfil en [twitter](https://twitter.com/mynameisjoaco) o [linkedin](https://www.linkedin.com/in/joaquin-varvasino-826819135/)

<!--chapter:end:index.Rmd-->


# Capítulo 2 

## Herramientas para el análisis distributivo {.unlisted .unnumbered}


#### Escrito por: Cristian Bonavida{-}
#### Last Update: 02/7/2021 {-}
<p>&nbsp;</p>

*Códigos escritos en base a los apéndices del libro "Pobreza y Desigualdad en América Latina" de Gasparini, Cicowiez y Sosa Escudero. El objeto de este material es reproducir la rutina de códigos para STATA presentada en el libro al lenguaje R. Este material es solo de caracter complementario a las explicaciones y detalles conceptuales que se presentan en el libro de texto y los apéndices* 

<p>&nbsp;</p>

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set Inicial {-}

Cargo las librerias, limpio environment, defino el path y atajo para función paste

```{r, message=FALSE}

library(dplyr)
library(tidyverse) # Data wrangling
library(tidygraph)
library(readxl)
library(ggplot2)
library(foreign)
library(Hmisc)

rm(list=ls())    #empiezo limpiando todo 

"%+%" <- function(x,y) paste(x,y,sep = "")      # defino un shorcut parar concat de texto
data_dir <- "C:/Users/HP/Desktop/CEDLAS - UNLP/Apendices en R/Material libro/encuestas-cedlas/Encuestas/"  #seteo directorio 

```

<p>&nbsp;</p>



## Introducción: ejemplo Brasil 

###- (pág. 72)

El siguiente código es introductorio y busca guiar al lector en la sintaxis básica a utilizar, mostrando a modo de ejemplo cómo replicar los resultados correspondientes al cuadro 2.1 del texto. 

Como será al inicio de cada capítulo, el primer paso es obtener base de datos, en ese caso la versión procesada de la PNAD (Pesquisa Nacional por Amostra de Domicílios) de Brasil para el año 2007, accesible desde el link de descarga indicado en la sección anterior: datos y set up básico. (Añadir sección y referir allí)

Una vez descargada la encuesta y guardada en el directorio que el lector indicó al definir el objeto *"data_dir"* se carga la encuesta que se encuentra en formato STATA con la librería `foreing`. Dentro de ella el comando `read.dta()` permite importar en R bases de datos que están en formatos compatibles con otros programas o lenguajes. Las encuestas de hogares procesadas que se utilizan a lo largo del libro solo deben contener observaciones que denominamos coherentes, que en pocas palabras se trata de observaciones válidas que utilizamos en el cálculo de los ingresos familiares. Por ello siempre que se cargue una base luego se debe filtrarla a partir de la columna *cohh*, para quedarse solo con estas observaciones.

Las encuestas de hogares, al igual que cualquier otra base de datos, se organizan en R como objetos tipo tabla que reciben el nombre de `dataframe`, donde las filas representan observaciones o registros y las columnas variables o campos. Con el comando `head()` podemos explorar las primeras observaciones de la encuesta. La expresión `[,1:9]` indica incluir solo de la columna 1 a 9 para que el output no sea demasiado extenso.


```{r, include=TRUE}
#cargo base
bra07 <- read.dta(data_dir %+% "Bra/2007/Bases/bra07_cedlas.dta") %>% 
         filter(cohh==1)

head(bra07[,1:9])

```

A su vez, por tratarse de una encuesta, cada observación representa a varios individuos, tantos como indica el factor de expansión o variable de ponderación. En nuestro caso, todas las encuestas que utilizaremos contienen una variable de nombre *pondera* que almacena el factor de expansión. Para más detalles sobre el uso de ponderadores, consultar más adelante la [sección 3.6](#cap-3.6)

Para expandir las observaciones por su peso muestral debemos emplear alguna de las librerías disponibles creadas para tal fin, ya que "por default" los comandos base de R no estiman estadísticos ponderados, como es el caso por ejemplo del comando `summary()`. Si se comparan los valores que arroja respecto a los valores de los mismos estadísticos pero ponderados, es evidente que existen diferencias (excepto obviamente para el mínimo y el máximo). Existen varias librerías que nos permiten obtener descriptivos ponderados, aquí se presentan los resultados con el uso de `Hmisc`, pero también puede explorarse de forma similar las librerías  `TAM` y  `srvy`.
   

```{r, include=TRUE}

#summary no arroja estadísticas descriptivas ponderadas
summary(bra07$ipcf)

```
    
```{r, include=TRUE}

#replicar la información que nos da summary pero ponderandola - usando libreria Hmisc
wtd.mean(bra07$ipcf, w=bra07$pondera)
wtd.quantile(bra07$ipcf, probs =c(0.25, 0.50, 0.75) , w=bra07$pondera)
min(bra07$ipcf, na.rm =T)
max(bra07$ipcf, na.rm =T)

```
Mientras que la media no pondera es de 559.1 al ponderar el valor estimado es 574.3
Lo mismo ocurre para la mediana, el primer y el tercer cuartil de la distribución. 

Una opción recomendada para usuarios más familiarizados con la sintaxis de R, es definir una función que nos calcule e imprima los resultados para estos estadísticos ponderados de una sola vez, replicando lo que hace `summary()`. Las funciones son una especie de comando personalizado que nos permiten customizar los cálculos y el output a nuestra necesidad o gusto. -ver [capitulo siguiente](#cap-3.2) donde nos extendemos sobre el uso de funciones-. En este caso nos permite elegir qué estadisticos queremos calcular y cómo mostrarlos, con una salida que incluye un título junto al resultado númerico mediante la función `paste()`) y redondeando a un decimal (mediante la función `round(..., d=1)`)


```{r, include=TRUE}

my_weighted_stats <- function(argumento1, argumento2){
  
  print(paste("Mean:", round( wtd.mean(argumento1, w=argumento2), d=1 )))
  print(paste("Sd:", round( sqrt(wtd.var(argumento1, w=argumento2)), d=1 )))
  print(round( wtd.quantile(bra07$ipcf, probs =c(0.25, 0.50, 0.75) , w=bra07$pondera), d=1 ))
  print(paste("Min:", round( min(argumento1, na.rm=TRUE), d=1 )))
  print(paste("Max:", round( max(argumento1, na.rm=TRUE), d=1 )))
  
}
```

Una vez definida la función debemos llamarla por el nombre indicando sus argumentos. En este caso el primero hace referencia a la variable y el segundo al ponderador. La función devuelve la información que arrojaría el comando `summ` en STATA ponderando las estimaciones.

```{r, include=TRUE}

my_weighted_stats(bra07$ipcf, bra07$pondera)

```


Una de las principales ventajas de las funciones es que nos permiten replicar los resultados para distintas encuestas o también para distintos subconjuntos de los datos. Por ejemplo, en el caso de la PNAD 2007 de Brasil, la variable *region* puede tomar los valores 1, 2, 3, 4 o 5 dependiendo de si la observación corresponde a la región Norte, Nordeste, Sudeste, Sur o Centro-Oeste, respectivamente. Así, las líneas siguientes pueden utilizarse para computar los estadísticos para dichas regiones, replicando aquí las columnas "Norte" y "Nordeste" del cuadro 2.1

```{r, include=TRUE}
my_weighted_stats(bra07$ipcf[bra07$region==1], bra07$pondera[bra07$region==1])
my_weighted_stats(bra07$ipcf[bra07$region==2], bra07$pondera[bra07$region==2])
```

Ahora el argumento no son todas las filas de las columnas *ipcf* y *pondera* sino solo aquellas que cumplen con la condición de pertenecer a la región 1 y 2 en cada caso, lo que se instrumenta con el subscript `[bra07$region==x]`. Esta expresión actúa como filtro, le indica a R considerar los valores de la variable ipcf y pondera que corresponden a observaciones que cumplen la condición sobre la región.

Siguiendo con el cuadro 2.1 del texto, si quisiéramos replicar el coeficiente de variación, ahora que ya sabemos cómo obtener estadísticas ponderadas, se vuelve muy sencillo. Solo basta con definir un objeto que se llame *cv* y que contenga el cociente entre la media y el desvío estándar.


```{r, include=TRUE}

cv=sqrt(wtd.var(bra07$ipcf, w=bra07$pondera)) / wtd.mean(bra07$ipcf, w=bra07$pondera)
cv
```

Y si quisiéramos estimarlo para una región en particular solo deberíamos aplicar el subscript de la misma forma que lo hicimos antes.

```{r, include=TRUE}

cv= sqrt(wtd.var(bra07$ipcf[bra07$region==1], w=bra07$pondera[bra07$region==1])) / 
         wtd.mean(bra07$ipcf[bra07$region==1], w=bra07$pondera[bra07$region==1]) 
cv
```

La primera columna del cuadro 2.1 del texto del libro mostraba además la población de referencia o número de observaciones expandidas. Para calcularlo basta con sumar la columna pondera, es decir sumar a cada persona aumentándola por su factor de expansión. De esta forma vemos que la población de referencia de la PNAD 2007 es 187 millones de personas aproximadamente. 

```{r, include=TRUE}

#número de observaciones expandidas
sum(bra07$pondera)

```

El ejemplo del texto finaliza con el cómputo de la pobreza en Brasil para el año 2007, utilizando una línea de pobreza de 130 reales mensuales. Una forma sencilla de computar la proporción de individuos con ingresos mensuales menores a 130 reales es mediante el bloque de código siguiente, donde primero se genera una variable binaria *pobre* a partir del comando `ifelse()`, Este comando asignará el valor 1 cuando el individuo no supere el umbral monetario y 0 en caso contrario. Luego se computa simplemente la media ponderada de esta variable binaria y se imprime el resultado redondeado a 2 decimales.

```{r, include=TRUE}

#identificar individuos pobres
bra07 <- bra07 %>% mutate(pobre=ifelse(ipcf<129.883, 1, 0)) 

## Total país
pobres_pais = wtd.mean(bra07$pobre, bra07$pondera)
print(paste("shr pobres=", round(pobres_pais, d=2)))
```

El mismo resultado podría obtenerse como el cociente de la suma ponderada de personas pobres sobre el total de población.

```{r, include=TRUE}

#otra forma
pobres_pais = sum(bra07$pondera[bra07$pobre==1], na.rm = TRUE)/sum(bra07$pondera)
print(paste("shr pobres=", round(pobres_pais, d=2)))

```

Para calcular la tasa de pobreza para una región en vez de todo el país, basta simplemente con filtrar las observaciones.

```{r, include=TRUE}
## Region Norte
pobres_norte = wtd.mean(bra07$pobre[bra07$region==1], bra07$pondera[bra07$region==1])
print(paste("shr pobres=", round(pobres_norte, d=2)))
```



## Histogramas

###- (pág 76) 

Siguiendo al apéndice del libro mostramos como generar una serie visualizaciones que nos ayudan a analizar la distribución de una variable.

Iniciamos con las instrucciones para graficar un histograma de la distribución del ipcf en México para el año 2006 (figuras 2.2 a 2.8 del texto). Al igual que en el ejemplo de Brasil, el lector puede obtener la ENIGH (Encuesta Nacional de Ingresos y Gastos de los Hogares) mexicana de 2006 desde los links de descarga. La misma también cuenta con las variables *ipcf* y *pondera* que utilizaremos en lo que resta de este apéndice. Luego de cargar la base nos aseguramos de trabajar solo con las observaciones coherentes y de igual manera eliminamos cualquier valor "missing" o cero de la variable de ingresos.

```{r, include=TRUE}

#cargo base
mex06 <- read.dta(data_dir %+% "Mex/2006/bases/mex06_cedlas.dta")  

#elimino observaciones incoherentes, con ingreso missing o cero
mex06 <- mex06 %>% filter(cohh==1 & !is.na(ipcf) & ipcf!=0) 

```

Para graficar optamos por la librería más extendida en uso en R, `ggplot2`, ya que tiene una enorme variedad de gráficos y posibilidades de personalización, pero al mismo tiempo mantiene un sintaxis común. Para graficar existen básicamente 3 ítems o funciones comunes que todos los gráficos deben tener y que aquí introducimos brevemente:

* __`ggplot()`__: Es la función que da inicio al gráfico, indicando que un objeto grafico va a generarse y consta de dos argumentos que debemos especificar siempre: 
  + data: como primer argumento debemos pasarle el dataframe o base de datos donde se encuentran las variables a graficar
  + aes: es el segundo argumento donde indicamos el o los ejes (x e y) y la diversificación de colores, relleno o formas según los grupos en los que se dividen los datos (por ejemplo si fuéramos a graficar líneas de tendencia y quisiéramos obtener una línea de color diferente para cada región, deberíamos especificarlo aquí)
* __`geom_...()`__: donde los puntos se completan con el tipo de grafico que queremos. Existe una función para cada gráfico. En el caso de un gráfico de líneas será por ejemplo `geom_line()` y en el caso de un histograma `geom_histogram()`.
* __customización__: existe múltiples funciones y casi infinitas combinaciones para adaptar el grafico en sus escalas,                       colores, títulos, dimensiones, fuente de los ejes, leyendas, faceteado, etc. 

A continuación, siguiendo estas líneas generales, graficamos un histograma para el ipcf, empleando la función `geom_histogram()`. Igual que antes, `[w=pondera]` indica que cada observación de la encuesta debe expandirse según la cantidad de individuos que representa. La opción `bins = 100` especifica que el histograma debe identificar 100 grupos (100 barras). Por último el termino `stat(count) / sum(count)` irá dentro de `aes()` ya que define al eje y, indicando que queremos un histograma en proporciones y no en valores absolutos. Los demás son pequeñas customizaciones de títulos y colores que el lector podrá intuir fácilmente.

```{r, include=TRUE, warning=FALSE, out.width = "80%"}

## Figura 2.2 - histograma ipcf
ggplot(data=mex06, 
       aes(x=ipcf, weight=pondera)) + 
  geom_histogram(bins = 100, aes(y = stat(count) / sum(count)),
                 color="black", fill="grey") +
  labs(y="proporción", x="ingreso per cápita familiar")

```

Con las líneas siguientes se filtra la base para individuos con ipcf menor a 1500 para evitar la típica distorsión generada por los valores extremos.

```{r, include=TRUE, warning=FALSE, out.width = "80%"}

## Figura 2.3 - histograma ipcf sin outliers
ggplot(data=mex06 %>% filter(ipcf < 15000), 
       aes(x=ipcf, weight=pondera)) + 
  geom_histogram(bins = 100, aes(y = stat(count) / sum(count)),
                 color="black", fill="grey") +
  labs(y="proporción", x="ingreso per cápita familiar")

```


Las figuras 2.4 y 2.5 del texto pueden replicarse utilizando el bloque de código siguiente. En la primera se grafica el logaritmo de la variable ipcf, agregando simplemente la función `log()` para el eje x. En la siguiente se incrementa sucesivamente el número de barras del histograma. Para ello podríamos repetir 4 veces el gráfico cambiando solamente la cantidad de bins. Aquí optamos por presentar un solución más elegante en la que generamos los 4 gráficos a partir de un bucle que itera sobre el número de bins deseado. El lector no familiarizado con los bucles puede optar por la primera opción hasta que en los siguientes capítulos presentemos con más detalle cómo se instrumentan iteraciones mediante bucles.


```{r, include=TRUE, warning=FALSE, out.width = "80%"}

## Figura 2.4 - histograma logaritmo ipcf
ggplot(mex06, 
       aes(x=log(ipcf), weight=pondera)) + 
  geom_histogram(bins = 100, aes(y = stat(count) / sum(count)),
                 color="black", fill="grey") +
  scale_x_continuous(breaks = seq(0,15,5))+
  labs(y="proporción", x="ingreso per cápita familiar")

```



```{r, include=TRUE, error=FALSE, message=FALSE, warning=FALSE, out.width = "90%"}

## Figura 2.5 - histogramas con diferente cantidad de intervalos
n_bins = c(10,50,100,1000)
my_graphs = list()

i=1
for (n in n_bins){
  
    my_graphs[[i]] <- ggplot(mex06, 
                           aes(x=log(ipcf), weight=pondera)) + 
                   geom_histogram(bins = n, aes(y = stat(count) / sum(count)),
                               color="black", fill="grey") +
                   scale_x_continuous(breaks = seq(0,15,5))+
                   labs(y="proporción", x="ingreso per cápita familiar") +
                   ggtitle( n %+% " intervalos") +
                   theme(plot.title = element_text(size = 12, hjust = 0.5))

  i=i+1  
}

g1 <-  my_graphs[[1]]
g2 <-  my_graphs[[2]]
g3 <-  my_graphs[[3]]
g4 <-  my_graphs[[4]]

library(gridExtra)
grid.arrange(g1, g2, g3, g4, ncol=2, nrow=2)

```

Las líneas que siguen grafican, superpuestos, los histogramas suavizados de las funciones de densidad del logaritmo del ipcf para las regiones Noroeste y Sur de México. Para este grafico, dentro de `aes()` debemos indicar no solo los ejes x e y junto con el ponderador, sino además debemos indicar que queremos que lo diferencia por grupos. Concretamente la opción `fill=factor(region)` le indica a R que el color de relleno de la curva ("fill") estará dado por una variable de la base de datos, en este caso la variable "región". Dado que dicha variable en la base es del tipo numérico, empleamos `factor()` para tratarla como una categórica. Para graficar una curva de densidad el comando indicado es `geom_density()`. Habiendo especificado estas opciones R mostrará una curva de densidad para cada región en el mismo gráfico, a partir de una misma variable (ipcf) ponderándola por el factor de expansión. 

Al código se agregan algunas customizaciones mas para la densidad: `color="black"` indica que queremos bordes negros, `alpha=0.4` indica el grado de transparencia en el relleno (para que al superponer las curvas continúen siendo visibles) y `size` indica el grosor del borde simplemente. Finalmente, dado que optamos por el color de relleno para diferenciar Noroeste y Sur, controlamos estas leyendas con la opción `scale_fill_discrete()`. Notar que en la primer línea definimos el objeto *"lp"* que contiene el valor de la línea de pobreza internacional de USD 2.5 por día por persona para México (en log), que marcamos como una línea vertical en el grafico con `geom_vline()`.


```{r, include=TRUE, warning=FALSE, out.width = "80%"}

## Figura 2.7 - histogramas superpuestos por regiones
lp=log(608.245)

ggplot(mex06 %>% filter(region==1 | region==6), 
       aes(x=log(ipcf), weight=pondera, fill=factor(region))) + 
  geom_density(color="black", alpha = 0.4, size=0.7) +
  scale_fill_discrete(name="Región", labels=c("Noroeste", "Sur")) + 
  geom_vline(xintercept=lp)  +
  labs(y="densidad", x="logaritmo del ingreso per c?pita familiar") 


```

Una de las ventajas de estos histogramas suavizados es facilitar las comparaciones. Como puede verse, las dos distribuciones son claramente diferentes. La distribución del Sur está desplazada a la izquierda, lo que sugiere que en general los individuos de esa región tienen menores ingresos que en el Noroeste y por lo tanto una densidad mayor caerá por debajo de la línea de pobreza fijada en esa región.


Por último, siguiendo la figura 2.6 y 2.8, se presenta un histograma suavizado junto con una distribución normal, con la diferencia de que aquí el histograma representa la densidad y no la frecuencia relativa. En R esta opción demanda algún ajuste extra en los ejes, que aquí a los fines didácticos se omite, pero que el lector podrá replicar fácilmente siguiendo la nota al pie.[^1]

```{r, include=TRUE, warning=FALSE, out.width = "80%"}

## Figura 2.6/2.8 - histograma logaritmo ipcf suavizado
ggplot(mex06, 
       aes(x=log(ipcf), weight=pondera)) + 
  geom_histogram(bins = 100, aes(y = ..density..), 
                 color="black", fill="gray")  +
  geom_density(kernel="gaussian", color="blue3", size=0.7) +  
  labs(y="densidad", x="logaritmo del ingreso per capita familiar")


```



## Función de distribución 

###- (pág. 78) 

En este apartado se muestra cómo pueden graficarse las funciones de distribución presentadas en la sección 2.3.2 del cuerpo principal del capítulo. El primer paso para construir una función de distribución es ordenar (de menor a mayor) las observaciones de la encuestas, según la variable de ingreso elegida, *ipcf* en nuestro caso. Esto es lo que hacemos en la primera línea. En la línea siguiente se crea la variable *shrpop* para almacenar la proporción relativa acumulada de la variable *pondera*, es decir el porcentaje acumulado de la población que representa cada observación junto con todas las anteriores. *shrpop*  tendrá entonces un valor de 100 para la última observación. Para obtener la suma acumulada expandida empleamos el comenado `cumsum()` y lo dividimos por el total de población expandido que se obtiene con `sum()`. La función de distribución presenta las variables *shrpop* e *ipcf* en los ejes vertical y horizontal, respectivamente, como se indica en `aes()`, y se se grafica simplemente con una línea, por lo que usamos el comando `geom_line()`, donde solo especificamos su tamaño o grosor.

```{r,include=TRUE, error=FALSE, warning=FALSE, out.width = "80%"}

#ordenar según ipcf
mex06 <- mex06 %>% arrange(ipcf)

#población acumulada ordenamiento ipcf
mex06 <- mex06 %>% mutate(shrpop=cumsum(pondera)/sum(pondera))

## Figura 2.9 - función de distribución acumulada
ggplot(mex06, aes(x=ipcf, y=shrpop))+
  geom_line(size=1.2) +
  labs(y="proporción", x="ingreso per cápita familiar")

```

Nuevamente, la cola superior larga de la distribución vuelve al gráfico poco útil. Para
aliviar este problema las alternativas son o bien truncar los valores superiores del ingreso, o
trabajar en logaritmos. La figura 2.10 muestra ambas alternativas. Para la primera, en vez de establacer una valor arbitrario a mano, calculamos el ingreso del percentil 95 y en base a este umbral filtramos las observaciones.

```{r,include=TRUE, error=FALSE, warning=FALSE, out.width = "80%"}

## Figura 2.10 función de distribución acumulada sin outliers
p95 = wtd.quantile(mex06$ipcf, probs=0.95, w=mex06$pondera)
trunca  <- ggplot(mex06 %>% filter(ipcf<p95), 
                aes(x=ipcf, y=shrpop))+
           geom_line(size=1.2) +
           ggtitle("Ignora 5% mas rico")+
           labs(y="proporción", x="ingreso per cápita familiar") 

logipcf <- ggplot(mex06, 
                  aes(x=log(ipcf), y=shrpop))+
           geom_line(size=1.2) +
           ggtitle("Toda la población") +
           labs(y="proporción", x="ingreso per cápita familiar")  


grid.arrange(trunca, logipcf, ncol=2)

```

Se deja como ejercicio para el lector elaborar las otras funciones de distribución presentadas en la sección 2.3.2. Por su parte, la curva de Pen (ver figuras 2.12 y 2.13) se construye igual que la función de distribución pero se grafica invirtiendo los ejes.

## Pareto 

###- (pág. 78)

En esta sección se muestra cómo replicar la figura 2.14 del texto, que muestra los diagramas de Pareto para las regiones Noroeste y Sur de México. El procedimiento es muy similar al caso de la función de distribución ya que en esencia representan lo mismo, de forma distinta. El gráfico de Pareto muestra para cada valor del ingreso x el porcentaje de la población que recibe ingresos superiores a ese valor x, en una escala doble logarítmica. El cambio de escala genera una suerte de zoom óptico sobre los estratos de mayores ingresos, permitiendo un examen más detallado de esa parte de la distribución.

Para graficarlo seguimos los pasos anteriores, pero ahora ordenamos a la población por ingreso dentro de cada región y ya no considerando el total país. Para ello agrupamos las observaciones por región con `group_by()` previo a calcular el share acumulado, de forma tal que este cálculo se haga solo entre individuos de una misma región. En la siguiente línea se genera la variable *lpareto* a partir de la variable *shrpop*, siguiendo la explicación de la sección 2.3.4 del texto. Finalmente se grafica filtrando la base para las regiones de interés y, siguiendo la misma lógica empleada en el grafico de densidad superpuesta, indicamos dentro de `aes()` la opción `linetype` para lograr un tipo de línea diferente que distinga a cada región. Notar que aquí también empleamos `factor()` para tratar esta variable categórica. Por último, con `scale_linetype()` customizamos las leyendas.

```{r,include=TRUE, error=FALSE, out.width = "80%"}

#población acumulada por región
mex06 <- mex06 %>% group_by(region) %>% mutate(shrpop=cumsum(pondera)/sum(pondera)) 

mex06 <- mex06 %>% mutate(lpareto=log(1-shrpop)) 

## Figura 2.14 - Diagrama de Pareto
ggplot(mex06 %>% filter(region==1 | region==6), 
       aes(x=log(ipcf), y=lpareto, weight=pondera, linetype=factor(region))) + 
  geom_line(size=1.2) +
  scale_linetype(name="Región", labels=c("Noroeste", "Sur")) + 
  labs(x="logaritmo del ingreso per cápita familiar")

```

Las líneas siguientes repiten el ejercicio pero dejando de lado al 1% más rico de la poblacion en cada región. Aquí en vez de filtrar en base al valor de ingresos, lo hacemos en base al porcentaje acumulado, como alternativa al caso anterior.

```{r,include=TRUE, error=FALSE, out.width = "80%"}

cutoff=0.99

ggplot(mex06 %>% filter((region==1 | region==6) & (shrpop<=cutoff)),
       aes(x=log(ipcf), y=lpareto, weight=pondera, linetype=factor(region))) + 
  geom_line(size=1.2) +
  scale_linetype(name="Región", labels=c("Noroeste", "Sur")) + 
  labs(x="logaritmo del ingreso per cápita familiar")

```


## Box-plot 

###- (pág. 79)

Aquí se muestra cómo elaborar diagramas de caja o box-plot como los presentados en la sección 2.3.5 del texto. En este caso haremos una excepción y presentamos primero una alternativa más directa a `ggplot` que permite manejar facilmente los outliers, a partir de la libreria base de R. El grafico 2.17 retoma la sintaxís habitual de ggplot para costruir un box-plot para cada región. El lector podrá entender rapidamente el código en ambos casos. En el gráfico conjunto, al igual que antes, diferenciamos a las regiones por el color de relleno en cada box, especificando la opción `fill` y empleamos `factor()` para tratar a la variable como categórica. Dado que en este caso la vaiable irá en el eje x diferenciando a las regiones, le agregamos las labels correspondientes con la función `scale_x_discrete()`. La opción `alpha=` la utilizamos para darle transparencia al relleno y `legend.position=none` para omitir la leyenda de cada color.

```{r,include=TRUE, error=FALSE, out.width = "80%"}

#Una opción mas directa fuera de ggplot

## Figura 2.15 - Box Plot excluyendo outliers
boxplot(mex06$ipcf, outline = FALSE)

## Figura 2.16 - Box Plot en log con outliers
boxplot(log(mex06$ipcf), outline = TRUE)

## Figura 2.17  - Box Plot en log por regiones con outliers
ggplot(mex06 %>% filter(region==1 | region==6), 
       aes(x=factor(region), y=log(ipcf), weight=pondera, fill=factor(region))) + 
  geom_boxplot(alpha=0.4) +
  theme(legend.position="none") + 
  scale_x_discrete(labels = c("Noroeste","Sur")) +
  labs(x="Región", y ="log ipcf")


```


## Curva de Lorenz 

###- (pág. 80) 

En este apartado se muestra cómo pueden construirse las curvas de Lorenz introducidas en la sección 2.3.6 del capítulo. El primer paso consiste en ordenar a los individuos de menor a mayor según su ingreso, en nuestro caso contenido en la variable ipcf. Para ellos aplicamos la función `arrange()`. Notar que previo a esto se aplica la función `ungroup()`, lo cual es siempre recomendable cuando en la misma base ya se había realizado una agrupación, para de alguna forma "resetear" ese agrupamiento previo. Las líneas siguientes generan la variable *shrpop* de la misma forma en la que fue generada más arriba, estimando la proporción de la población que se acumula hasta cada observación de la encuesta. La diferencia es que ahora también generaremos una variable *shrinc* que contiene la proporción del ingreso que se acumula hasta cada observación y que se estima expandiendo el ingreso por el ponderador, es decir multiplicando ambas columnas, como puede verse en el código. 

La curva de Lorenz nos muestra qué porcentaje de la población acumula un dado porcentaje del ingreso total. Para visualizarlo indicamos entonces los ejes respectivos para  *shrpop* y  *shrinc* y luego aplicamos un gráfico de línea con `geom_line()`.


```{r,include=TRUE, error=FALSE, out.width = "80%"}

#ordenar según ipcf
mex06 <- mex06 %>% ungroup() %>% arrange(ipcf)

#población e ingreso acumulado
mex06 <- mex06 %>% mutate(shrpop=cumsum(pondera)/sum(pondera),
                          shrinc=cumsum(ipcf*pondera)/sum(ipcf*pondera)) 

## Figura 2.18 - curva de Lorenz 
ggplot(mex06, aes(x=shrpop, y=shrinc)) +
  geom_line()

```

En las líneas siguientes se comparan las curvas de Lorenz para dos regiones de México. El código sigue los mismos pasos que antes pero ahora los cálculos de población e ingreso acumulado se realizan por región, agregando un `group_by()`. En el gráfico filtramos las regiones en cuestión y las diferenciamos con distintos tipos de líneas con la opción *linetype*, al igual que antes. Con `theme_bw()`, elegimos una temática diferente para modificar un poco el aspecto visual del gráfico.

```{r,include=TRUE, error=FALSE, out.width = "80%"}


#ordenar según ipcf + región y acumular por región
mex06 <- mex06 %>% arrange(region, ipcf) %>% 
                   group_by(region) %>% 
                   mutate(shrpop=cumsum(pondera)/sum(pondera),
                          shrinc=cumsum(ipcf*pondera)/sum(ipcf*pondera)) 

## Figura 2.19 - curva de lorenz por regiones
ggplot(mex06 %>% filter(region==1 | region==6),
       aes(x=shrpop, y=shrinc, linetype=factor(region))) +
  geom_line() +
  scale_linetype(name="Region", labels=c("Noroeste", "Sur")) +
  theme_bw()

```


## Curva Generalizada de Lorenz 

###- (pág. 81) 

La curva generalizada de Lorenz se construye a partir de la curva de Lorenz pero multiplicando su eje vertical por el ingreso promedio (ver sección 2.3.6 en el cuerpo del capítulo). Las líneas de código no se modifican respecto de las utilizadas para estimar la curva de Lorenz, salvo en que ahora la variable *shrpop* la generamos sobre la base de la variable *pondera*. Para entender el álgebra detrás de esta forma de calcular la curva generalizada de Lorenz, el lector puede remitirse a los apéndices del libro (pagina 81-82) donde queda claramente explicitado. Recordar que la curva generalizada de Lorenz muestra el ingreso acumulado en el x% más pobre de la población, sobre el número de personas. Como quedará mas claro en los capítulos 6 y 7, mientras que la curva de Lorenz se emplea para estudiar desigualdad, la generalizada de Lorenz es muy útil para analizar bienestar agregado. 

```{r,include=TRUE, error=FALSE, out.width = "80%"}

#ordenar según ipcf + región y acumular por región
mex06 <- mex06 %>% arrange(region, ipcf) %>% 
                   group_by(region) %>% 
                   mutate(shrpop=cumsum(pondera)/sum(pondera),
                          glorenz=cumsum(ipcf*pondera)/sum(pondera)) 

## Figura 2.20 - curva de Lorenz generalizada por regiones
ggplot(mex06 %>% filter(region==1 | region==6),
       aes(x=shrpop, y=glorenz, linetype=factor(region))) +
  geom_line() +
  scale_linetype(name="Region", labels=c("Noroeste", "Sur")) +
  theme_bw()

```



## Curva de Incidencia {#cap-2.8}

###- (pág. 83)

En este apartado se muestra cómo pueden estimarse las curvas de incidencia del crecimiento que aparecen en la figura 2.21 del texto. A modo de ejemplo, se computa la curva de incidencia del crecimiento para Argentina entre 1992 y 2006, utilizando percentiles del ingreso per cápita familiar. Para ello el lector deberá descargar dichas bases en su directorio de trabajo.

La idea básica será generar un bucle que en cada repetición cargue una base, la ordene por ipcf, genere el porcentaje de población acumulado, en base a este identifique el percentil de ingreso  que corresponde y finalmente estime la media del ingreso en percentil. Dado que tenemos dos bases distintas, el bucle iterará dos veces. 

Para implementarlo se propone utilizar una lista (objeto tipo *list*), en la que la EPH de cada año va a ser un elemento distinto. Así el primer elemento de esa lista corresponderá a toda la base de 1992 y el segundo, a toda la base del 2006. Para señalar el primer y segundo elemento nos valemos de un contador *i* que en la primer iteración del bucle valdrá uno, y al que iremos sumando de uno en uno, por lo que en la segunda tomará valor dos. Por eso la expresión `bases_mod[[i]]` será equivalente a decir "elemento 1 de la lista *bases_mod*" en la primera iteración y "elemento 2 de la lista *bases_mod*" en la segunda iteración. Con esto logramos que en vez de ordenar, generar y cambiar variables sobre un dataframe estático, lo hagamos sucesivamente sobre dataframes distintos contenidos en un lista "madre", que aquí llamamos *bases_mod*. 

Nuestro bucle irá iterando sobre *j*, que al igual que *i* cambiará de valor en cada vuelta. En la primera será 92 y en la segunda 06 y nos servirá para indicar con qué base trabajar. Así `read.dta()` tiene como argumento la función `paste()` que une el prefijo *"arg"* al año de la base correspondiente (*j*), generando, junto con el prefijo *"dta"*, un string igual al nombre de la base completa. De esta forma logramos cambiar el nombre en cada iteración para cargar bases de años diferentes. 

Dado que estamos trabajando con años distintos debemos prever que necesitaremos ajustar los valores de ingresos por inflación. Por eso en caso de que la base sea la del 92, se multiplica el ipcf por el factor de ajuste (2.099). Esto se debe hacer solo en el caso de la base de 1992 y la manera de instrumentarlo es diciéndole a R que ese cambio debe realizarse solo cuando j sea igual a 92 -`if (j=="92")`-.

Una vez cargada la base, logrado el ajuste por inflación, aplicamos los mismos pasos que antes para ordenar por ipcf a la población y calcular el porcentaje acumulado. Con ese porcentaje acumulado generaremos los percentiles anidando un bucle a nuestro bucle inicial, que hará 100 repeticiones en cada EPH. En cada una de ellas asignará los percentiles en base a *shrpop* ayudándose de la variable *z* multiplicada por 0.01. La razón es sencilla: si queremos generar percentiles (n=100), necesitamos 100 cuantiles por lo que cada cuantil se asigna de a intervalos de población acumulada iguales a 0.01 (1/100). Así por ejemplo, cuando z sea igual a 20, `(z-1)*0.01` será igual a 0.19 y `z*0.01` a 0.20, por lo que caerán en este cuantil veinte, todos aquellos individuos que ordenados por ingreso, estén entre el 19 y el 20 por ciento de población acumulada. Dado que empleamos el comando `ifelse()`, para las obserservaciones restantes que caen fuera de este intervalo, se mantendrá el valor que ya traía la variable *"percentil"*. Es por esto que antes del bucle, la generamos como vacía, para luego ir rellenándola sucesivamente. *z* continuará incrementándose de a uno, hasta llegar a 100, donde asigne el último percentil a aquellas personas que quedaron ultimas en nuestra ordenación, ya que poseían los ingresos mas alto. Al considerar a este 1% de la población faltante, se habrá acumulado al 100%. Finalmente el último paso, consiste en reducir todo el dataframe a 100 observaciones correspondientes a los 100 percentiles recién creados y calcular la media del ingreso de las personas que pertenezcan a cada percentil.


```{r,include=TRUE}

bases_mod <- list() #lista vacia en la que se guardaran los cambios

i <- 1              #contador para iterar sobre los elementos de la lista

#j tomará valor 92 y 06
for (j in c("92","06")){     
  
  #cargo base
  bases_mod[[i]] <- read.dta(data_dir %+% paste("Arg/bases/arg",j,"_cedlas.dta", sep ="")) %>% 
                    filter(cohh==1 & !is.na(ipcf) & ipcf!=0)
  
  #ajuste por inflación para el año 92 solamente
  if (j=="92"){
    
      bases_mod[[i]] <- bases_mod[[i]] %>% mutate(ipcf=ipcf*2.099)
    
  }
  
  #ordenar según ipcf
  bases_mod[[i]] <- bases_mod[[i]] %>% arrange(ipcf)
  
  #computar porcentaje de población
  bases_mod[[i]] <- bases_mod[[i]] %>% mutate(shrpop=cumsum(pondera)/sum(pondera))
  
  #identificar percentil de ipcf
  bases_mod[[i]] <- bases_mod[[i]] %>% mutate(percentile=0)  #esto es equivalente a gen percentile=. en stata
   
      for (z in 1:100){  #bucle anidado: itera 100 veces para cada una de las bases 
    
        bases_mod[[i]] <- bases_mod[[i]] %>% mutate(percentile=ifelse((shrpop>(z-1)*0.01 & shrpop<=z*0.01), z, percentile)) 
        
      }  
  
  bases_mod[[i]] <- bases_mod[[i]] %>% group_by(percentile) %>% 
                                       summarise(ipcf=wtd.mean(ipcf, w=pondera, na.rm=TRUE))
  
  i=i+1  
  
}

```


Cuando el bucle finaliza, podemos "recuperar" como dataframes los elementos almacenados en nuestra lista *"bases_mod"*. El primero corresponde a los ingresos promedios de cada percentil construidos con la base de 1992 y el segundo con la de 2006.

```{r,include=TRUE}

#recupero las bases de cada año como respectivos elementos de la lista
arg92 <- data.frame(bases_mod[[1]])
arg06 <- data.frame(bases_mod[[2]])

head(arg92)
```

Para finalizar la estimación, debemos calcular el cambio porcentual en el ingreso promedio de cada percentil entre estos años. Para ello necesitaremos juntar ambos dataframes en un solo, a partir de la variable percentil, que se encuentra en ambos. Con el comando `merge()` unificamos las bases y le asignamos un sufijo que nos permita identificar cada columna. Hecho esto ya podemos calcular el cambio porcentual en el ipcf promedio y graficarlo para cada percentil.

```{r,include=TRUE,  out.width = "80%"}

#junto bases, asigno nombres y calculo el cambio porcentual
change <- merge(arg92, arg06, by="percentile", suffix=c("_92", "_06")) %>% 
          mutate(change=( (ipcf_06/ipcf_92) -1)*100) 


## Figura 2.21 - Curva de incidencia ipcf
ggplot(change,
       aes(x=percentile, y=change))+
  geom_line(size=1.2, color="darkcyan") +
  geom_hline(yintercept = 0, linetype="dashed") +
  theme_bw()
```

En el caso particuar de esta figura, las magnitudes son diferentes a los reportados orginalmente en el capitulo del libro, pero las interpretaciones se mantienen. La tasa de crecimiento del ingreso
real está completamente por debajo del eje horizontal, denotando una pérdida de ingresos reales, que dada la pendiente positiva de la curva implica caídas proporcionales del ingreso más grandes a medida que vamos descendiendo hacia estratos más pobres de la distribución. Es claro que la desigualdad de ingresos debe haber aumentado en Argentina en ese período.



[^1]: Para explorar una posible manera de plotear histogramas de frecuencia relativa o absoluta junto con un densidad suavizada el lector encontrará util la [siguiente entrada](https://stackoverflow.com/questions/27611438/density-curve-overlay-on-histogram-where-vertical-axis-is-frequency-aka-count) de *stackoverflow*.

<!--chapter:end:02-Cap2_html.Rmd-->



# Capítulo 3 

## Variables y fuentes de información {.unlisted .unnumbered}


#### Escrito por: Cristian Bonavida{-}
#### Last Update: 02/7/2021{-}
<p>&nbsp;</p>

*Códigos escritos en base a los apéndices del libro "Pobreza y Desigualdad en América Latina" de Gasparini, Cicowiez y Sosa Escudero. El objeto de este material es reproducir la rutina de códigos para STATA presentada en el libro al lenguaje R. Este material es solo de caracter complementario a las explicaciones y detalles conceptuales que se presentan en el libro de texto y los apéndices* 

<p>&nbsp;</p>


## Set Inicial {-}

Cargo las librerias, limpio environment, defino el path y atajo para función paste

```{r, message=FALSE}

library(dplyr)
library(tidyverse) # Data wrangling
library(tidygraph)
library(readxl)
library(ggplot2)
library(foreign)
library(TAM)

rm(list=ls())    #empiezo limpiando todo 

"%+%" <- function(x,y) paste(x,y,sep = "")      # defino un shorcut parar concat de texto
data_dir <- "C:/Users/HP/Desktop/CEDLAS - UNLP/Apendices en R/Material libro/encuestas-cedlas/Encuestas/"  #seteo directorio 

```

<p>&nbsp;</p>


## Cociente de cuantiles 

###- (pág. 151-152) 

El siguiente bloque de código puede utilizarse para computar el cociente de quintiles extremos presentado en el cuadro 3.2 del texto del libro, el cual es un indicador de desigualdad extendido en la literatura, que denota la magnitud de las brechas entre los más ricos y más pobres. Para iniciar se importa la base desde el formato STATA utilizando de la librería `foreing`, el comando `read.dta()`. Luego hago una limpieza de la base, ordeno las observaciones según la variable "ipcf", filtro ingresos nulos y por último computo el porcentaje acumulado por población, como se vio en el capítulo anterior.

```{r, include=TRUE}
#cargo base
mex06 <- read.dta(data_dir %+% "Mex/2006/bases/mex06_cedlas.dta")  

#elimino observaciones incoherentes o con ingreso missing
mex06 <- mex06 %>% filter(cohh==1, !is.na(ipcf)) 

#ordenar por ipcf, filtrar ingresos nulos y computar % de población
df <-  mex06  %>% arrange(ipcf) %>% filter(ipcf>0) %>% 
       mutate(shrpop=cumsum(pondera)/sum(pondera))

```

A partir de allí genero la variable quintil en el dataframe, que vale 1 para el 20% más pobre de la población, 2 para el 20% siguiente, y así sucesivamente. El comando `ifelse` permite evaluar el porcentaje acumulado de población en cada observación y le otorga el valor del quintil correspondiente en caso afirmativo, o deja la variable inalterada en caso negativo. La lógica detrás es idéntica a la vista para generar percentiles dentro del bucle en la estimación de las curvas de incidencia ([sección 2.8](#cap-2.8)).

```{r, include=TRUE}

df$quintil = 0
df$quintil = ifelse(df$shrpop<=0.2, 1, df$quintil)
df$quintil = ifelse(df$shrpop> 0.2 & df$shrpop<= 0.4, 2, df$quintil)
df$quintil = ifelse(df$shrpop> 0.4 & df$shrpop<= 0.6, 3, df$quintil)
df$quintil = ifelse(df$shrpop> 0.6 & df$shrpop<= 0.8, 4, df$quintil)
df$quintil = ifelse(df$shrpop> 0.8 & df$shrpop<= 1,   5, df$quintil)

```


Para terminar computamos el ingreso promedio ponderado para las observaciones del quintil 1 y 5. Aquí la expresión *`[df$quintil==1]`* actúa como filtro. De esta manera el comando le pide a R considerar los valores de la variable *ipcf* y *pondera* que corresponden a observaciones que cumplen la condición, en este caso de pertenecer al quintil 1. Por último calculamos el ratio de estos dos valores e imprimimos el resultado. Para mayor claridad siempre se recomienda redondear los valores a imprimir, utilizando el comando `round()`.

```{r, include=TRUE}
ipcf_q1 = weighted.mean(df$ipcf[df$quintil==1], df$pondera[df$quintil==1], na.rm = TRUE)
ipcf_q5 = weighted.mean(df$ipcf[df$quintil==5], df$pondera[df$quintil==5], na.rm = TRUE)
ratq51 = ipcf_q5/ipcf_q1

paste("ratq51 = ", ratq51)
paste("ratq51 = ", round(ratq51, digits = 2))

```

El valor obtenido indica que el ingreso en el quintil 5 es 13.4 veces más alto que el del quintil 1, para el año 2006 en México considerando el total país.


<p>&nbsp;</p>


## Replicar programa ratq51 {#cap-3.2}

###- (pág. 153-154) 

En la siguientes líneas de código proponemos escribir una función que permite computar el cociente de quintiles extremos muy fácilmente; simplemente se lo invoca indicando la base de datos, la variable de la que se quieren computar quintiles, y opcionalmente la variable de ponderación y la condición `if`. Una función en R es similar a lo que en STATA llamaríamos un programa, es decir un conjunto de instrucciones que se guardan en la memoria y que pueden ser fácilmente replicables en distintas bases de datos y para distintas variables. 

La función puede resultar un poco extensa por lo que aquí enumeraremos las líneas de código para mayor claridad. Para iniciar la función, en la primer línea debemos especificar sus argumentos, que en este caso son: un dataframe (objeto *"df"*), una variable (objeto *"varname"*) y opcionalmente una variable de ponderación y una condición (objetos *"var_pondera"* y *"condicion"*). Para llamar a esta función entonces siempre tendremos que, como mínimo, especificar los dos objetos iniciales.

La siguiente línea se asegura que la base de datos *"df"* que hemos especificado siempre sea almacenado como un objeto del tipo *data frame* en un objeto llamado *"aux"* que creamos en el entorno de la función. La noción de entorno es importante para comprender el resto del código. A los fines de nuestro código diremos que un entorno es un espacio determinando en el que se almacenan distintas clases de objetos que R acepta y, en el cual estos tienen un significado, un valor o simplemente existen como objetos. En R pueden "coexistir" environmets distintos, a los cuales se le asigna un nivel o prioridad[^2]. Volviendo a nuestro código, cuando a R le especificamos, por ejemplo, el objeto *"varname"*, R buscará si este está definido en el *entorno o environment* general del programa como prioridad, y no en el entorno o espacio de la función, que es más acotado que el anterior. Por lo tanto aquí necesitamos evitar este comportamiento y lograr que R no busque evaluar al objeto por fuera del ambito de la función. El comando `substitute()` le indica a R que considere al objeto con su nombre literal. Así cuando *"varname"* sea definida, por ejemplo, como la variable "ipcf", R no buscará a qué es equivalente el objeto "ipcf" en su memoria sino que tomará el nombre dado como literal. Luego la función `eval()` le indica a R que ese nombre debe evaluarlo, es decir buscarlo o identificarlo, dentro del dataframe *"aux"* que hemos definido en nuestra función. De esta forma nos aseguramos de que R considera correctamente las variables dentro de la base de datos que estamos especificando, y por lo tanto que nuestros argumentos sean válidos dentro del ámbito de la función. Es decir que no entren en conflicto con otros objetos similares o definidos previamente en el environment más amplio del programa. 

Utilizando estas dos funciones, en la línea 7 almacenamos todos los valores de ipcf a partir de los cuales luego ordenamos la base en la siguiente línea. Como la condición es un argumento opcional debemos evaluar si esta fue indicada, en términos de R si no es un objeto nulo (línea 12). Si no lo es, luego en la línea 14 verificamos que esté correctamente especificada, escribiendo una sentencia que será VERDADERA cuando al evaluar de forma silenciosa la condición, dé como resultado un error *("try-error")*. En ese caso entonces se imprime un mensaje que ayuda al usuario a identificar donde se encuentra el problema. En caso de que el resultado no sea del tipo error, la condición será aceptada por lo que filtramos el objeto *"aux"* de acuerdo a la especificación dada. Para ello nos valemos de la función `parse` que convierte la condición que era del tipo *"string"* al tipo *"expression"*, lo que la vuelve compatible de ser evaluada como sentencia, nuevamente con la función `eval()` dentro de `filter()`.

De forma similar procedemos con el ponderador en la línea 24. Si no fue especificado (objeto nulo), entonces vale 1 para todas las observaciones, a partir de crear una variable que llamamos "w" que pasará a ser nuestra variable ponderador, pero que será inocua a los fines del cálculo. En el objeto *"var_pondera_store"* almacenamos esta secuencia de valores 1. En cambio, si el ponderador fue especificado entonces ese objeto almacena los valores correspondientes a la variable señalada, sin entrar en conflicto con un posible objeto del mismo nombre definido fuero del espacio de la función.

En la línea 38 actualizamos el objeto que almacena los valores de ipcf (en caso de que se hayan filtrado valores de acuerdo a la condición), y en la siguiente ordenamos el dataframe *"aux"*, para calcular como antes el porcentaje acumulado de población. Por último definimos inicialmente a la variable "quintil" como valores 0. En la línea 45 replicamos la asignación del quintil que se realizó antes con el comando `ifelse` pero ahora de forma iterativa, cambiando en cada iteración los valores del quintil y de población acumulada.

Por último al igual que antes calculamos el ratio entre quintiles extremos filtrando las observaciones del primer y quinto quintil. La única diferencia es que aquí nuevamente debemos asegurarnos de que los valores promedios y los ponderadores se evalúen para estas observaciones y sólo en el contexto o espacio de la función. Para finalizar imprimimos los resultados.



```{r, attr.source='.numberLines'}

ratq51 <- function(df, varname, var_pondera=NULL, condicion=NULL) {
    
    aux <- as.data.frame(df) 
    
    #substitute() toma literal el nombre sin asignarle ningun valor posible y 
    #eval() evalua ese nombre en el contexto del data frame que es el primer arguento definido
    varname_store <- eval(substitute(varname), aux) 
    aux <- aux %>% arrange(varname_store)
    
    
    #la condición es un argumento opcional
    if(!is.null(substitute(condicion))) {
      
       if(is(try(eval(substitute(condicion)), silent = TRUE ), "try-error"))
        
          stop("ERROR: la condicion debe especificarse como character (entre comillas)")
      
          aux <- aux %>% filter(eval(parse(text=condicion)))  #convierte la condicion de tipo string al tipo 
                                                            #expression y luego la evalua dentro del filter
    }
    
    
    #set pondera= 1 si no está especificado
    if(is.null(substitute(var_pondera))) {                           
      
      aux <- aux %>% mutate(w=1)
      var_pondera=substitute(w)
      var_pondera_store <- eval(substitute(var_pondera), aux)
      
      } else {
      
      var_pondera_store <- eval(substitute(var_pondera), aux)
      
    }
    
    
    #ordeno y genero quintiles
    varname_store <- eval(substitute(varname), aux)
    aux <- aux %>% arrange(varname_store) %>%
                   mutate(shrpop=cumsum(var_pondera_store)/sum(var_pondera_store)) %>% 
                   mutate(quintil=0)
    
    for (i in 1:5) {
      
     aux <- aux %>% mutate(quintil=ifelse((shrpop>(i-1)*0.2 & shrpop<=i*0.2), i, quintil)) 
      
    }  
    
    
    #ingreso promedio quintil 1 
    aux_1 <- aux %>% filter(quintil==1) 
    media_q1 = weighted.mean(eval(substitute(varname),aux_1), eval(substitute(var_pondera), aux_1), na.rm = TRUE)
    media_q1 = round(media_q1, digits = 2)
    #ingreso promedio quintil 5 
    aux_5 <- aux %>% filter(quintil==5) 
    media_q5 = weighted.mean(eval(substitute(varname),aux_5), eval(substitute(var_pondera), aux_5), na.rm = TRUE)
    media_q5 = round(media_q5, digits = 2)
    #ratio
    ratq_51 = media_q5/media_q1
    
    
    print(paste("media quintil 5", substitute(varname), "= ", media_q5))
    print(paste("media quintil 1", substitute(varname), "= ", media_q1))
    print(paste("ratio = ", round(ratq_51, digits = 2)))
    
  
}
```


Al correr el código, R almacenará en la memoria esta función, a la cual podremos llamar simplemente indicando los argumentos relevantes. En las líneas siguientes se detallan algunos ejemplos de cómo replicar la función bajo distintas especificaciones.


```{r, include=TRUE, error=TRUE}

#No especifica condicion
ratq51(mex06, ipcf, pondera)  
#Especifica condicion
ratq51(mex06, ipcf, pondera, "ipcf>0") 
#Especifica condición doble
ratq51(mex06, ipcf, pondera, "region==4 | region==2")  
#Especifica incorrectamente la condición
ratq51(df=mex06, ipcf, pondera, region==4 & urbano==1) 
#No especifica pondera
ratq51(df=mex06, varname=ipcf, condicion="region==4 & urbano==1")  
#Identifica los argumentos explicitamente por nombre 
ratq51(df=mex06, varname=ipcf, var_pondera=pondera, condicion="region==4 & urbano==1") 

```


<p>&nbsp;</p>



## Replicar programa gcuan 

###- (pág. 154-155) 

El bloque de código a continuación permite identificar cuantiles de cualquier variable. En términos del programa "ratq51", nos permite generar variables similares a quintil pero que pueden identificar quintiles, deciles, ventiles, percentiles, etc. 

Por esta razón, esta función tendrá más argumentos, aquí además de los anteriores debemos detallar la cantidad de cuantiles a generar (objeto *"num"*) y la variable que los almacena (*"newvar"*). Luego el código y la secuencia son idénticos a la de la función anterior, salvo porque aquí definimos de una manera alternativa y más directa a los valores del poderador, pero en esencia replica lo visto antes hasta la línea 40. A partir de aquí el objeto *"num"* indica cuantos cuantiles deben generarse, hace iterar al bucle *"num"* cantidad de veces, y define los intervalos de población acumulada de forma equivalente. Por ejemplo, si queremos generar deciles (num=10), necesitamos 10 cuantiles y cada cuantil se asigna de a intervalos de población acumulada iguales a 0.10 (1/10).

Posteriormente en la línea 50 generamos el reporte (objeto *"show"*) que imprimiremos como resultado. Este calcula la media ponderada, el desvió standard ponderado y la cantidad de observaciones en base la variable especificada en *"varname"* y *"var_pondera"*. Los objetos *"my_var"* y *"my_var2"* son variables auxiliares que generamos en el data frame solo con el objeto de facilitarnos el cálculo directo. Para terminar cambiamos el nombre de la variable que hasta ahora generamos como "quintil" por el nombre indicado en el argumento. El comando `names(aux)` trae todos los nombres de este dataframe y con la expresión *`[names(aux) == "quintil"]`* elegimos de esos nombres, solo el que coincide con la palabra "quintil". A esta columna específica le asignamos el nombre dado como argumento, tomando la expresión literal con `substitute()` pasada al formato string mediante la función `paste()`. En resumen la línea 59 sería el equivalente de escribir en la consola *`aux$quintil <- "nombre_asignado"`*, con la dificultad de que debemos hacerlo dentro del espacio de la función, respetando los argumentos dados. La línea 60 por su parte elimina del output final la variable shrpop.

Para finalizar especificamos que esta función no solo debe imprimirnos los resultados guardados en *"show"*, sino que además debe devolver una base de datos nueva. El comando `return(aux)` le dice a R que el resultado será el objeto *"aux"* en sí mismo, es decir obtendremos la base original con una nueva columna llamada *"newvar"* que es la que esta función genera.

```{r attr.source='.numberLines'}


gcuan <- function(df, varname, var_pondera=NULL, condicion=NULL, num, newvar) {
  
  aux <- as.data.frame(df) 
  
  varname_store <- eval(substitute(varname), aux) 
  aux <- aux %>% arrange(varname_store)
  
  
  #la condición es un argumento opcional
  if(!is.null(substitute(condicion))) {
      
     if(is(try(eval(substitute(condicion)), silent = TRUE ), "try-error"))
        
        stop("ERROR: la condicion debe especificarse como character (entre comillas)")
      
        aux <- aux %>% filter(eval(parse(text=condicion)))     
                                                              
   }
  
  
  #set pondera= 1 si no está especificado
  if(is.null(substitute(var_pondera))) {                           
    
    var_pondera_store <- c(rep(1, nrow(aux)))
    
    } else {
    
    var_pondera_store <- eval(substitute(var_pondera), aux)
    
  }
  
  
  #ordeno y genero shrpop
  varname_store <- eval(substitute(varname), aux)
  aux <- aux %>% arrange(varname_store) %>% 
    mutate(shrpop=cumsum(var_pondera_store)/sum(var_pondera_store)) %>%
    mutate(quintil=0)
  
  #genero cuantiles en base a lo indicado
  shareq = 1/num
  
  for (i in 1:num) {
    
    aux <- aux %>% mutate(quintil=ifelse((shrpop>(i-1)*shareq & shrpop<=i*shareq), i, quintil))
    
  }
  
  
  #armo la información que se imprime en la consola
  show <- aux %>% mutate(my_var=varname_store,
                         my_var2=var_pondera_store) %>% group_by(quintil) %>%
                  summarise(mean = weighted_mean(my_var, my_var2),
                             std = weighted_sd(my_var, my_var2),
                             obs = sum(my_var2))
  
  
  #renombro variable al nombre indicado como argumento y descarto shrpop
  names(show)[names(show) == "quintil"] <- paste(substitute(newvar))
  names(aux)[names(aux) == "quintil"] <- paste(substitute(newvar))
  aux$shrpop <- NULL
  
  #outputs
  print.data.frame(show)
  return(aux)
  
  
}
```

A diferencia entonces de la anterior, esta función devuelve no sólo un resultado impreso sino un objeto, por lo que ahora debemos especificar cómo lo nombramos. Si lo llamamos de la misma manera que el argumento que le pasamos a la función simplemente estamos pisando este objeto y añadiéndole una nueva variable. Otra alternativa es crear un nuevo dataframe con otro nombre.

```{r, include=TRUE, error=TRUE}

#Especifica todo correctamente y crea nuevo dataframe
mex06_bis <- gcuan(df=mex06, varname=ipcf, var_pondera=pondera, condicion="ipcf>0", num=5, newvar=quintil)

#No especifica correctamente la condición
mex06_bis <- gcuan(df=mex06, varname=ipcf, var_pondera=pondera, condicion=region==4, num=5, newvar=quintil)

#No especifica condición
mex06_bis <- gcuan(df=mex06, varname=ipcf, var_pondera=pondera, num=5, newvar=quintil)

#No especifica pondera
mex06_bis <- gcuan(df=mex06, varname=ipcf, num=5, newvar=quintil)

#Especifica todo correctamente y reemplaza el dataframe dado
mex06 <- gcuan(df=mex06, varname=ipcf, var_pondera=pondera, condicion="ipcf>0", num=5, newvar=quintil)

```


<p>&nbsp;</p>


## Tamaño de los hogares 

###- (pág. 156) 

El código siguiente puede utilizarse para computar las estadísticas sobre proporción de hogares unipersonales y multipersonales presentadas en el cuadro 3.4 del texto. Con este código podremos calcular qué proporción del total de hogares se compone de 1, 2, 3, 4,…n miembros y combinado con los códigos anteriores, analizar cómo esta configuración cambia al agrupar por regiones, percentil de ingreso, condición de pobreza, etc. Como ya es habitual, en la primera línea indicamos con qué base iremos a trabajar. Luego ordenamos los hogares de forma creciente en base a su identificador *"id"* y en forma decreciente respecto la variable *"jefe"*. Dado que esta vale 1 solo para el jefe de hogar y 0 para todo el resto, la primera observación por *"id"* corresponderá siempre a la cabeza del hogar. La función `duplicated()` genera una variable que será *FALSE* para la primera observación dentro de cada id, indicando que esta no está duplicada por ser la primera, pero será *TRUE* para todas las que siguen, ya que se identificó antes una observación con el mismo id. De esta manera hacemos un "tag" del jefe de hogar y mantenemos todos los demás miembros. En el caso de que no nos interesa mantener en nuestra base el resto de las observaciones, estos pasos se simplifican a un simple filtrado por jefe de hogar.

Luego generamos la variable tamaño solo para el jefe de hogar en base la cantidad de miembros que viven con él, utilizando la función `case_when()`. Así, por ejemplo, cuando la observación corresponda al jefe y la cantidad de miembros sea igual a 3, en ese caso la variable *"tamaño"* valdrá 3. Por último armamos nuestra tabla resultado filtrando a los jefes de hogar, agrupando por tamaño y sumando la cantidad de hogares (porque tenemos una sola observación por hogar) y la frecuencia relativa en cada caso.


```{r, include=TRUE}

#indico con qué base de hogares voy a trabajar
df <- mex06 

#ordeno por id y jefe (decreciente) e identifico al jefe de hogar
df <- df %>% arrange(id, desc(jefe)) %>%
          mutate(hh=duplicated(id))      #idéntico a: egen hh=tag() en stata

#genero tamaño sólo para los jefes de hogar
df <- df %>% mutate(tamaño=case_when(miembros==1 & hh==FALSE ~ 1,
                                     miembros==2 & hh==FALSE ~ 2,
                                     miembros==3 & hh==FALSE ~ 3,
                                     miembros==4 & hh==FALSE ~ 4,
                                     miembros==5 & hh==FALSE ~ 5,
                                     miembros>=6 & hh==FALSE ~ 6))

#tabla con resultados
table <- df %>% filter(hh==FALSE) %>% group_by(tamaño) %>% summarise(N = sum(pondera)) %>% 
                                                           mutate(freq = N*100/sum(N))
table

```

El objeto table almacena los resultados que se visualizan en la consola. A veces al imprimir los resultados estos no se visualizan con un formato muy amigable. A modo de extensión presentamos dos alternativas para refinar este aspecto. El paquete `formattable` nos permite, entre otras cosas, especificar el tipo de datos y el formato para distintas variables al generar el dataframe. Por su parte `print.data.frame` nos da una visualización más limpia. Incorporando este aspecto el código para nuestra tabla sería:

```{r, include=TRUE}

#install.packages("formattable")
library(formattable)

table <- df %>% filter(hh==FALSE) %>% group_by(tamaño) %>% summarise(N = accounting(sum(pondera), digits = 0)) %>% 
                                                              mutate(freq = percent(N/sum(N)))
print.data.frame(table)

```

Además de agregarle esta customización, para los usuarios que utilizan "Rmarkdown" habitualmente, es posible mostrar los resultados con una visualización amigable de forma sencilla.

```{r, include=TRUE}

rmarkdown::paged_table(table)

```


<p>&nbsp;</p>


## La distribución intrahogar 

###- (pág. 157)

El fragmento de código siguiente puede utilizarse para generar resultados similares a los presentados en el cuadro 3.7 del texto, que muestra cómo se modifica la desigualdad calculada a través del cociente de deciles extremos cuando cambia la distribución del ingreso hacia el interior del hogar. Cabe recordar que la distribución del ingreso intrahogar se modifica mediante un impuesto proporcional al ingreso per cápita familiar combinado, con un subsidio que solo recibe el jefe de hogar. En la implementación, utilizamos quintiles en lugar de deciles ingreso.

En la primera línea cargamos la base, en este caso de Venezuela año 2006, limpiamos y ordenamos los hogares. Más abajo creamos el objeto *"ty"* que toma el valor de la tasa del impuesto aplicada sobre el ipcf, el cual calculamos en la línea siguiente generando en el dataframe una nueva columna *"impuesto"*. Posteriormente calculamos el valor del subsidio a otorgar, que surge de sumar el valor de los impuestos para los integrantes de un mismo hogar. En la línea siguiente modificamos la variable subsidio, haciendo que valga 0 para todo miembro distinto al jefe de hogar, de esta forma redistribuyendo el ingreso al interior del hogar. A todos se le hemos quitado una porción *"ty"* que ahora la recibe solamente el jefe.  

En la anteúltima línea creamos una variable para el nuevo valor de ingreso per cápita familiar, restando el impuesto y sumando el subsidio. Por último hacemos uso de nuestra función del ratio de quintiles para computar el cociente del ingreso promedio de los quintiles 5 y 1 como indicador de desigualdad, a partir del ingreso modificado.


```{r, include=TRUE}

ven06 <- read.dta(data_dir %+% "Ven/2006/bases/ven06_cedlas.dta")  

#elimino observaciones incoherentes o con ingreso missing y ordeno
df <- ven06 %>% filter(cohh==1, !is.na(ipcf)) %>% arrange(id)

#tasa del impuesto
ty=0.1

#impuesto al ipcf
df$impuesto = df$ipcf*ty    #alternativa a escribir: df <- df %>% mutate(impuesto = ipcf*ty)

#recaudación impuesto total por hogar
df <- df %>% group_by(id) %>% mutate(subsidio = sum(impuesto))

#subsidio solo lo recibe el jefe de hogar 
df$subsidio <- ifelse(df$jefe!=1, 0, df$subsidio)  

#nuevo ipcf
df$ipcf_star = df$ipcf - df$impuesto + df$subsidio 

ratq51(df, ipcf_star, pondera, "ipcf>0")  

```

A modo de comentario, un usuario mas familiarizado con el lenguaje podrá notar que las lineas que computan y asignan el subsidio puden resumirse en una sola, escribiendo.

```{r}

df <- df %>% group_by(id) %>% mutate(subsidio=ifelse(jefe!=1, 0, sum(impuesto)))

```


<p>&nbsp;</p>


## Empleo de ponderadores {#cap-3.6}

###- (pág. 157)

El bloque de código que sigue puede utilizarse para construir un cuadro como el 3.9 del texto, que muestra la relación entre el ingreso per cápita familiar y el valor de la variable de ponderación.

Dado que ya habíamos trabajado con la base de México, aquí no la volvemos a cargar. En la segunda línea  genera la variable *"shrobs"*, que contiene el porcentaje acumulado de observaciones, y que es distinta de *"shrpop"* que contiene el acumulado de población usando el ponderador. Aquí la expresión *`1:n()`* enumera secuencialmente las observaciones en orden de aparición mientras que *`n()`* computa el total. El resto es habitual, generamos quintiles en base a los cuales luego agrupamos las observaciones para calcular la media de ingreso y el total de población en cada uno.


```{r, include=TRUE}

#indico con que base de hogares voy a trabajar
df <- mex06

#ordenar por ipcf y computo porcentaje acumulado de observaciones
df <- df %>% arrange(ipcf) %>%  mutate(shrobs = 1:n()/n()) 

#identificar quintiles de ipcf
df$quintil = 0
df$quintil = ifelse(df$shrobs<= 0.2, 1, df$quintil)
df$quintil = ifelse(df$shrobs>0.2 & df$shrobs<= 0.4, 2, df$quintil)
df$quintil = ifelse(df$shrobs>0.4 & df$shrobs<= 0.6, 3, df$quintil)
df$quintil = ifelse(df$shrobs>0.6 & df$shrobs<= 0.8, 4, df$quintil)
df$quintil = ifelse(df$shrobs>0.8 & df$shrobs<= 1,   5, df$quintil)

show <- df %>% group_by(quintil) %>% summarise(mean = accounting(mean(ipcf), digits = 0),
                                               means_w = accounting(mean(pondera), digits = 0))
print.data.frame(show)

```


Seguidamente se calculan las tasas de pobreza con y sin ponderadores para el total del país y para cada una de las regiones de México en 2006, correspondientes al cuadro 3.10 del texto. En la primer línea el objeto *"lp"* almacena el valor de la línea de pobreza, en base a la cual se genera la variable binaria *"pobre"*, que vale 1 para los individuos debajo de este umbral (es decir, si *ipcf < lp*) y 0 para el resto. La suma de esta variable arroja el número total de personas pobres en la muestra. Al multiplicarla por el ponderador obtenemos el número de personas pobres en la población y dividiendo por la población total del país nos devuelve la tasa de incidencia de la pobreza. En este caso el valor es de 13.57%.

```{r, include=TRUE}

###linea de pobreza us$2.5 Mexico 2006
lp= 633.90918

#identificar individuos pobres
df$pobre = ifelse(df$ipcf<lp, 1, 0)

#total pais
sum(df$pobre*df$pondera)*100/sum(df$pondera)
```


Las siguientes líneas calculan el mismo valor pero de una manera alternativa, almacenándolo en un dataframe, a partir de contabilizar la frecuencia absoluta y relativa de personas pobres y no pobres.

```{r, include=TRUE}

pobreza <- df %>% group_by(pobre) %>% summarise(n = accounting(sum(pondera), digits = 0)) %>% 
                                      mutate(tasa_pobreza = percent(n/sum(n))) 
print.data.frame(pobreza)

```

Para replicar el mismo cálculo pero para las regiones del país, generamos un bucle que itera 8 veces, una vez por cada región. En cada vuelta del bucle se estima, de la misma forma que arriba, el valor de pobreza tanto ponderado como no ponderado. Para este segundo caso, el único paso adicional consiste en fijar la variable *pondera* como igual a 1. Como vimos en la salida anterior de la tabla, en la 2da fila de la 3er columna se encuentra la tasa de incidencia de pobreza. Con la expresión *`pobreza_pondera[2,3]`* llamamos a este valor para guardarlo en el objeto *"share_p"*, luego de redondearlo a dos dígitos con el comando `round()`. Lo mismo hacemos para las estimaciones sin ponderar. Por último, imprimimos los resultados acompañados por una leyenda indicativa.

```{r, include=TRUE}

#por region
for (i in 1:8){
  
  pobreza_pondera <- df %>% filter(region==i) %>% 
                            group_by(pobre) %>% summarise(n=sum(pondera)) %>% 
                                                mutate(tasa_pobreza=n/sum(n))
  
  pobreza_s_pondera <- df %>% filter(region==i) %>%
                              mutate(pondera=1) %>% 
                              group_by(pobre) %>% summarise(n=sum(pondera)) %>%
                                                  mutate(tasa_pobreza=n/sum(n))
  
  #recupero el valor llamando a la fila y columna y redondeo 
  share_p = round(pobreza_pondera[2,3]*100, digits = 2)
  share_sp = round(pobreza_s_pondera[2,3]*100, digits = 2)
  
  print(paste("H_ponderado =", share_p, "/ H_sin_ponderar =", share_sp))
  
}

```


La salida que obtenemos muestra un punto importante sobre el diseño de encuestas que impacta en nuestras estimaciones: Las tasas de pobreza suelen ser menores cuando se incluyen los ponderadores, dado que los factores de expansión son en general superiores en los estratos de mayores ingresos, donde las tasas de no respuesta son más elevadas.

El bloque de códigos anterior imprime los resultados en la consola. A modo de extensión, si quisiéramos generar un cuadro u objeto que los almacene (un *dataframe*) para luego, por ejemplo, exportarlo en Excel u otro formato, solo debemos modificar ligeramente el código de arriba. Antes de correr el bucle generamos un dataframe que contiene todos valores iguales a 0, pero con 3 columnas (region, pondera, sin_pondera). Al final del bucle guardamos los datos de la región, del valor de pobreza con ponderador y sin ponderador en la columna 1, 2 y 3 respectivamente. La clave es que en cada iteración lo hacemos en una fila distinta, que corresponde a cada región. Por ejemplo en la tercera iteración el código filtra la región a la cual le corresponde el valor 3, y almacena las estimaciones de línea de pobreza en la tercera fila de la tabla *"results"*.



```{r}

#creo data frame solo con ceros, de 8 filas y 3 columnas
results <- data.frame(region=c(rep(0,8)),
                      pondera=c(rep(0,8)),
                      sin_pondera=c(rep(0,8)))

#por region
for (i in 1:8){
  
  pobreza_pondera <- df %>% filter(region==i) %>% 
                            group_by(pobre) %>% summarise(n=sum(pondera)) %>% 
                                                mutate(tasa_pobreza=n/sum(n))
  
  pobreza_s_pondera <- df %>% filter(region==i) %>%
                              mutate(pondera=1) %>% 
                              group_by(pobre) %>% summarise(n=sum(pondera)) %>%
                                                  mutate(tasa_pobreza=n/sum(n))
  
  #recupero el valor llamando a la fila y columna y redondeo 
  results[i,1] = paste("region", i)
  results[i,2] = round(pobreza_pondera[2,3]*100, digits = 2)
  results[i,3] = round(pobreza_s_pondera[2,3]*100, digits = 2)
  
}

```

De esta manera al finalizar el bucle queda armado un objeto de 3 columnas con 8 filas, una para cada región.

```{r, echo=FALSE}

rmarkdown::paged_table(results)

```

<p>&nbsp;</p>


## Diseño muestraL 

###- (pág. 160)
 
En este material no se cubre con ejemplos este apartado pero para los usuarios interesados en adentrarse en el manejo de encuestas de hogares contemplando el diseño muestral, se recomienda explorar el uso del paquete "SURVEY", similar al paquete "svy" en STATA.

```{r, eval=FALSE}

#install.packages("survey")
library(survey)
?survey

```

<p>&nbsp;</p>



## Fuentes de ingreso 

###- (pág. 161)

El bloque de código a continuación muestra cómo computar la importancia que tiene cada fuente de ingresos identificada en las encuestas de hogares (cuadro 3.13). Dentro de las fuentes de ingreso consideramos: laboral (variable *ila*), jubilaciones (*ijubi*), capital (*icap*), transferencias (*itran*) y otros (*ionl*).

Para comenzar declaramos la base y la limpiamos. Luego generamos la variable ingreso total (*itot*), como la suma de las columnas para cada ingreso. Hasta ahora todas las veces que aplicamos el comando `sum()` lo hicimos sumando una misma columna, por ejemplo para obtener el total de población, sumamos la variable *pondera*. En este caso queremos sumar por fila, distintas columnas. Para eso debemos anteponer el comando `rowwise()`.

Como resultado final necesitamos mostrar la participación en el ingreso total de cada fuente de ingreso. Para ello debemos considerar cada fuente por separado y divirla por el ingreso total. La forma de hacerlo nuevamente es utilizando un bucle. Entonces el código a escribir debe lograr en cada iteración tomar todos los valores de cada una de las variables que corresponde a la fuente de ingresos, en otras palabras debemos iterar entre columnas distintas. La forma que proponemos aquí consiste en valernos de las listas. Las listas, como ya vimos, son un objeto que en cada elemento puede almacenar otro objeto de cualquier tipo. Una lista de n elementos puede contener n dataframes distinto en cada uno de ellos, o n columnas de un dataframe, o n vectores. Aquí entonces nos valdremos de esta flexibilidad para almacenar en un mismo objeto (lista) múltiples objetos distintos (columnas)

La cuarta línea de códigos declara a *"ingresos"* como una lista que como elementos contiene a las variables de ingreso de nuestra base de datos. En la línea siguiente creamos un simple vector con los nombres de cada una de estas. Con esto ya podemos generar nuestro bucle, haciéndolo iterar desde 1 hasta n, siendo n la cantidad de fuentes de ingresos distintas que consideramos. Para evitarnos contar manualmente cuantas son, directamente calculamos n con el comando `length()` que nos devuelve la cantidad de elementos dentro de la lista "ingresos". De esta forma si agregamos o quitamos una variable de ingreso no debemos preocuparnos por fijar este valor cada vez. 

En la primer línea del código definimos la variable *y* que, en cada iteración, será un elemento distinto del objeto ingreso, es decir una fuente de ingreso distinta, comenzando por *ila* y terminando en *ionl*. A esta variable la expandimos multiplicándola por el ponderador, de la misma manera que expandimos la variable de ingreso total, que ya calculamos más arriba. Solo nos resta hacer el cociente entre la suma de los ingresos de todas las personas correspondiente a cada fuente, sobre el total de todos los ingresos. El objeto *"value"* almacena este cálculo, que luego redondeamos a dos dígitos. Nótese la importancia de la opción *`na.rm=TRUE`* que evita que un valor missing convierta en missing a toda la suma, es decir que indica ignorar los valores missings y preservar el cálculo sobre el resto de las observaciones no missings. Para terminar, "concepto" almacena el nombre de la fuente correspondiente a cada vuelta del bucle, para imprimirlo en la línea final junto con el valor calculado.


```{r}

arg06 <- read.dta(data_dir %+% "Arg/2006/s2/bases/arg06_cedlas.dta") 

df <- arg06 %>% filter(cohh==1, !is.na(ipcf)) 

#sumo los ingresos por fila (rowwise)
df <- df %>% rowwise %>% mutate(itot = sum(ila, ijubi, icap, itran, ionl, na.rm = TRUE))

#creo una lista en la que cada elemento es un vector distinto de ingreso
ingresos <- list(df$ila, df$ijubi, df$icap, df$itran, df$ionl)
names <- c("laboral", "jubilación", "capital", "transferencias", "otros")


#itero sobre cada uno de esos vectores 
for (i in 1:length(ingresos)){
  
  y = ingresos[[i]]
  y_expand = y * df$pondera
  itot_expand = df$itot * df$pondera
  
  value <- sum(y_expand, na.rm=TRUE) / sum(itot_expand, na.rm=TRUE) * 100
  value <- round(value, digits = 2)
  
  concepto = names[i]
  print(paste("shr %", concepto, "= ", value))


  }

```



[^2]: Para los usuarios que no esten tan familiarizados con las nociones de *Environment* y *Non-Standard Evaluation* se recomienda revisar las siguientes referencias para un tratamiento mas detallado.

    *http://adv-r.had.co.nz/Environments.html#environments
    
    *http://adv-r.had.co.nz/Computing-on-the-language.html
    
    *https://advanced-r-solutions-ed1.netlify.app/non-standard-evaluation.html#non-standard-evaluation-in-subse

<!--chapter:end:03-Cap3_html.Rmd-->


# Capítulo 4 

## Pobreza monetaria {.unlisted .unnumbered}


#### Escrito por: Cristian Bonavida {-}
#### Last Update: 28/8/2021 {-}

<p>&nbsp;</p>

*Códigos escritos en base a los apéndices del libro "Pobreza y Desigualdad en América Latina" de Gasparini, Cicowiez y Sosa Escudero. El objeto de este material es reproducir la rutina de códigos para STATA presentada en el libro al lenguaje R. Este material es solo de caracter complementario a las explicaciones y detalles conceptuales que se presentan en el libro de texto y los apéndices* 

<p>&nbsp;</p>


## Set Inicial  {-}

Cargo las librerias, limpio environment, defino el path y atajo para función paste

```{r, message=FALSE}

library(dplyr)
library(tidyverse) # Data wrangling
library(tidygraph)
library(readxl)
library(ggplot2)
library(foreign)
library(TAM)

rm(list=ls())    #empiezo limpiando todo 

"%+%" <- function(x,y) paste(x,y,sep = "")      # defino un shorcut parar concat de texto
data_dir <- "C:/Users/HP/Desktop/CEDLAS - UNLP/Apendices en R/Material libro/encuestas-cedlas/Encuestas/"  #seteo directorio 

```

<p>&nbsp;</p>


## Indicador FGT  

###- (PÁG. 249-250) 


En este apartado se presenta cómo calcular la familia de indicadores FGT. En primer lugar, se muestra cómo puede computarse el indicador FGT de manera relativamente sencilla. Luego, al igual que con el ratio de quintiles y el cálculo de quintiles, se introduce una función para el indicador FGT. Como ejemplo, computamos la pobreza de 2.5 dólares para Ecuador en 2006, utilizando microdatos que provienen de la Encuesta de Condiciones de Vida (ENCOVI).

Luego de cargar y limpiar las bases fijamos los valores de pobreza así como el parámetro *"alfa"* del indicador fgt de aversión a la desigualdad entre los pobres. Luego se computa, para cada individuo pobre, su brecha de pobreza elevada al valor de *"alfa"*, que se asigna a una nueva variable que llamamos *each*. La distinción entre individuo pobre y no pobre se operativiza en el comando `ifelse()`: en caso de cumplirse la condición de que el ingreso esté por debajo de la línea de pobreza, se computa la brecha, en caso de que sea falso se otorga un valor de cero. Por último se obtiene el valor de FGT como el promedio ponderado de la brecha en toda la muestra.



```{r, include=TRUE}

#cargo base y elimino observaciones incoherentes o con ingreso missing
ecu06 <- read.dta(data_dir %+% "Ecu/2006/bases/ecu06ecv_cedlas.dta") %>% 
         filter(cohh==1, !is.na(ipcf))     

df <- ecu06 

#linea de pobreza
lp=39.74
#parametro alfa indicador fgt
alfa=0

#computar fgt
df <- df %>% mutate(each = ifelse(ipcf<lp, (1 - ipcf/lp)^alfa, 0 ))   
fgt = weighted.mean(df$each, df$pondera, na.rm = TRUE)*100             

print("fgt = " %+% round(fgt, d=2))

```


Otra opción posible es computar los valores directamente como un vector, sin alojarlo como una nueva columna del data frame. Esta opción, desde el punto de vista del código, es más eficiente ya que se evita agregar una nueva columna que solo se emplea para el cálculo del indicador. El detalle a considerar es que para poder calcularlo como medida ponderada la cantidad de elementos del vector *"each"* debe ser exactamente igual la cantidad de observaciones de la columna *pondera* del dataframe.


```{r, include=TRUE}
#alternativa
each <- ifelse(df$ipcf<lp, (1 - df$ipcf/lp)^alfa, 0 )                
fgt = weighted.mean(each, df$pondera, na.rm = TRUE)*100   

print("fgt = " %+% round(fgt, d=2))
```

<p>&nbsp;</p>


## Computar FGT 

###- (pág 250-251)

Como ya vimos en el capítulo 3, las funciones nos permiten replicar un indicador o un cálculo en cualquier base, sobre cualquier variable, imponiendo una condición especifica o ponderando por algún factor de expansión. En este caso además agregamos como argumentos los parámetros asociados al FGT, *"alfa"*, para la aversión a la desigualdad, y *"zeta"* para la línea de pobreza. De esta forma podremos replicar y comparar rápidamente el índice para distintos valores que podemos pasarle a estos argumentos. La estructura de la función replica lo visto en las funciones del capítulo 3 hasta la línea 39 donde se computa el FGT con las mismas líneas que empleamos arriba. Adicionalmente se añade la opción de correr la función de forma silenciosa, sin imprimir el resultado.

```{r attr.source='.numberLines'}

FGT <- function(df, varname, var_pondera=NULL, condicion=NULL, alfa, zeta, quiet=FALSE) {
  
  aux <- as.data.frame(df) 
  
  varname_store <- eval(substitute(varname), aux) 
  aux <- aux %>% arrange(varname_store)
  
  
  #la condición es un argumento opcional
  if(!is.null(substitute(condicion))) {
      
     if(is(try(eval(substitute(condicion)), silent = TRUE ), "try-error"))
        
        stop("ERROR: la condicion debe especificarse como character (entre comillas)")
      
        aux <- aux %>% filter(eval(parse(text=condicion)))     
                                                              
   }
    
  
  #set pondera igual a 1 si no está especificado
  if(is.null(substitute(var_pondera))) {                           
    
    var_pondera_store <- c(rep(1, nrow(aux)))
    
  } else {
    
    var_pondera_store <- eval(substitute(var_pondera), aux)
    
  }
  
  
  #Cómputo de brecha y valor del indicador
  varname_store <- eval(substitute(varname), aux)
  aux <- aux %>% mutate(each = ifelse( varname_store < zeta, 
                                     ( 1 - varname_store/zeta)^alfa, 
                                       0 ))
    
  fgt = weighted.mean(aux$each, var_pondera_store, na.rm = TRUE)*100
  fgt = round(fgt, digits = 2)

  #output
  if(substitute(quiet)==TRUE){
    a=fgt
    
  } else {
    
    print(paste("FGT(alfa=", alfa, ",Z=", zeta, ") = ", fgt, sep=""))
    a=fgt
    
  }
  
}

```

En las líneas siguientes se detallan algunos ejemplos de cómo replicar la función bajo distintas especificaciones.

```{r, include=TRUE, error=TRUE}

#No especifica condicion
FGT(df=df, varname=ipcf, var_pondera=pondera, alfa=0, zeta=39.740)
#Especifica condicion
FGT(df=df, varname=ipcf, var_pondera=pondera, condicion="urbano==1", alfa=0, zeta=39.740)
#Especifica incorrectamente la condición
FGT(df=df, varname=ipcf, var_pondera=pondera, condicion=urbano==1, alfa=0, zeta=39.740)
#Especifica opcion "quiet"
FGT(df=df, varname=ipcf, var_pondera=pondera, alfa=0, zeta=39.740, quiet=TRUE)
```

<p>&nbsp;</p>


## Pobreza relativa 

###- (pág. 251)

La estimación de la pobreza relativa implica, como primer paso, el cálculo de una línea de pobreza relativa. A modo de ejemplo, se computa una línea de pobreza igual al 50% del ingreso mediano de Ecuador. El comando `weightedMedian()` de la librería `matrixStats` arroja el valor de la mediana. Luego, el cálculo de la pobreza se realiza empleando la función FGT.

```{r, include=TRUE}

#linea de pobreza del 50% de la mediana del ingreso
lp = matrixStats::weightedMedian(df$ipcf, df$pondera) * 0.50
FGT(df=df, varname=ipcf, var_pondera=pondera, condicion="urbano==1", alfa=0, zeta=lp)


```

<p>&nbsp;</p>



## Descompoisición regional de la pobreza 

###- (pág. 252-253)

El código a continuación realiza una descomposición por regiones de la tasa de incidencia de la pobreza (cuadro 4.7 del libro de texto). Para este caso utilizamos la Encuesta Nacional de Ingresos y Gastos de los Hogares de México para el año 2006 con la línea de pobreza de 2.5 dólares, equivalentes a 608.245 pesos mensuales. 

Luego de cargar, limpiar e indicar la base de datos con la que se trabajará, se fija la línea de pobreza y se genera, a partir de la función de FGT, la tasa de incidencia para el total del país que se almacena en el objeto *"p0"*. En la línea siguiente, se toma una única observación de cada una de las 8 regiones diferentes de México y se las ordena en orden creciente para almacenarlas como un vector en el objeto *"list_rgn"*. Sobre este objeto haremos iterar un bucle, para que en cada vuelta los cálculos indicados se hagan para cada una de estas regiones. 

La primer sentencia dentro de ese bucle, calcula la participación de la región sobre la población y en la siguiente línea se estima su indicador FGT para un valor de alfa=0 y con la línea de pobreza de 2.5 dólares (notar como se instrumenta la condición en la función). Por último la contribución se calcula como el producto entre la participación de cada región en la población total y el cociente entre la tasa de pobreza regional y la tasa de pobreza nacional. Se redondea y se indica que se imprima dicho valor.

 
```{r, include=TRUE}

#cargo y limpio base
mex06 <- read.dta(data_dir %+% "Mex/2006/bases/mex06_cedlas.dta") %>%
         filter(cohh==1, !is.na(ipcf))

df <- mex06

lp=608.245
p0 = FGT(df=df, varname=ipcf, var_pondera=pondera, alfa=0, zeta=lp, quiet=TRUE)

list_rgn = sort(unique(df$region))


for (i in list_rgn){
  
  #participación region
  shr_rgn = sum(df$pondera[df$region==i]) / sum(df$pondera)
  #fgt region
  p_r <- FGT(df=df, varname=ipcf, var_pondera=pondera, condicion=paste("region==", i, sep = ""), alfa=0, zeta=lp, quiet=TRUE)
  
  #contribución
  contribut = round( shr_rgn*(p_r/p0)*100 , digits = 2 ) 
  print(paste("contribución (%) region", i, "=", contribut))
  
  
}


```

<p>&nbsp;</p>


## Pobreza según consumo e ingreso 

###- (pág. 253-254)

El código que sigue puede utilizarse para replicar los resultados sobre pobreza por consumo e ingreso presentados en el cuadro 4.9 del texto para el caso de Nicaragua en 2005. Luego de fijar la tasa de pobreza relevante, el bucle hace que el objeto *”i”* tome valores desde 0.5 hasta 1.5 a intervalos de 0.1. Estos valores expanden la línea de pobreza que se utiliza para calcular el FGT sobre consumo (variable *cpcf*) e ingreso (variable *ipcf*)

```{r, include=TRUE}

nic05 <- read.dta(data_dir %+% "Nic/2005/bases/nic05_cedlas.dta") %>% 
         filter(coh_oficial==1)

df <- nic05 

#linea de pobreza oficial
lp0=576.5028


for (i in seq(0.5,1.5, by=0.1)) {
  
  #linea de pobreza
  lp=lp0*i; lp=round(lp, d=2)
  
  #consumo
  print(paste(i, "*lp = ", lp, "- Consumo", sep=""))
  FGT(df=df, varname=cpcf, var_pondera=pondera, alfa=0, zeta=lp)
  
  #ingreso
  print(paste(i, "*lp = ", lp, "- Ingreso", sep=""))
  FGT(df=df, varname=ipcf, var_pondera=pondera, alfa=0, zeta=lp)  
  
}

```

En este caso el código imprime una larga lista de resultados en la consola. Con un cambio menor en el bucle, con las mismas sentencias, *podemos generar directamente la tabla a replicar y almacenarla como un dataframe que luego es exportable fácilmente a otros formatos para su presentación*. Para ello generamos inicialmente 3 vectores vacíos para cada una de las columnas que tendrá la tabla. Utilizamos un contador auxiliar que irá incrementándose de a uno en cada iteración del bucle, en las cuales iremos guardando los datos que antes se imprimían, ahora como elementos de estos objetos. Así, por ejemplo en la tercera iteración el cálculo de FGT se guardará como el tercer elemento del vector consumo y el vector ingreso. Al finalizar creamos un dataframe de 3 columnas a partir de estos 3 vectores, y por ultimo agregamos la cuarta columna calculando la diferencia.

```{r}

consumo <- c()
ingreso <- c()
linea_pobreza <- c()

j = 1

for (i in seq(0.5,1.5, by=0.1)) {
  
  #linea de pobreza
  lp=lp0*i; lp=round(lp, d=2)
  
  linea_pobreza[j] <- paste(i, "*lp", sep="")

  #consumo
  consumo[j] <- FGT(df=df, varname=cpcf, var_pondera=pondera, alfa=0, zeta=lp, quiet = TRUE)
  #ingreso
  ingreso[j] <- FGT(df=df, varname=ipcf, var_pondera=pondera, alfa=0, zeta=lp, quiet = TRUE)  
  
  j=j+1
  
}

tabla <- data.frame(linea_pobreza, consumo, ingreso) %>% mutate(diferencia = ingreso - consumo)

```

```{r, echo=FALSE}

rmarkdown::paged_table(tabla)

```


El código siguiente permite replicar la figura 4.13 del texto, que compara las funciones de distribución del ingreso y el consumo per cápita. Para ello se ordena de forma creciente por las variables de ingreso y luego de consumo, calculando en cada caso el share de población. El *"cutoff"* se utiliza para indicar qué porcentaje de las observaciones se mostrará en el gráfico.

```{r, include=TRUE}

##FUNCIÓN DE DISTRIBUCIÓN ACUMULADA

#ordenar según ipcf y calcular shrpop
df <- df %>% arrange(ipcf) %>% mutate(shrpop_i = cumsum(pondera)/sum(pondera))
#ordenar según cpcf y calcular shrpop
df <- df %>% arrange(cpcf) %>% mutate(shrpop_c = cumsum(pondera)/sum(pondera))

cutoff=0.95

ggplot(df %>% filter(shrpop_i < cutoff), aes(x=ipcf, y=shrpop_i, linetype="Ingreso"))+
    geom_line(size=1.2) +
    #como la condición es sobre otra variable tengo que volver a indicar dataframe y aesthetic 
    geom_line(data = df %>% filter(shrpop_c < cutoff), aes(x=cpcf, y=shrpop_c, linetype="Consumo"), size=1.2) +
    scale_linetype_manual(name = "Variable", values=c(Ingreso="solid", Consumo="twodash")) +
    labs(y="proporción población", x="")
```


<p>&nbsp;</p>

## Pobreza por edad 

###- (pág. 254-255)

El bloque de código siguiente muestra cómo puede graficarse la relación entre pobreza y edad (ver figura 4.15 del texto). Para ello luego de cargar, limpiar la base y definir la línea de pobreza, generamos dos objetos *"x"* e *"y"* vacíos donde se almacenará los valores del eje x y el eje y. Estos valores se generaran de forma iterativa mediante un bucle que incremente secuencialmente la edad en 5 años, comenzando en 0 y terminando en 80 años. En cada vuelta estaremos generando el indicador FGT condicionando a las observaciones que caigan dentro de distintos intervalos de edad. Esto equivale a dividir a la población en grupos de edad y para cada uno de ellos calcular el indicador. Al igual que en el código anterior, el contador nos permite almacenar los valores de edad y FGT como elementos de los vectores creados inicialmente.


```{r, include=TRUE}

#indico con qué base de hogares voy a trabajar
mex06 <- read.dta(data_dir %+% "Mex/2006/bases/mex06_cedlas.dta") %>% 
         filter(cohh==1, !is.na(ipcf))
df <- mex06 

#linea de pobreza oficial
lp0=608.245

x <- c()
y <- c()

j=1

for (i in seq(0,80,by=5)) {
  
  print(paste("rango = [", i, ",", i+4, "]", sep=""))
  fgt_edad = FGT(df=df, varname=ipcf, var_pondera=pondera, condicion=paste("edad>=", i, " & edad<=", (i+4), sep=""), 
                 alfa=0, zeta=lp, quiet = TRUE)
  
  x[j]=i
  y[j]=fgt_edad
  
  j=j+1
  
}

```

Las líneas finales grafican los resultados, superponiendo a las estimaciones de pobreza una línea de regresión polinomial de orden dos.
.
```{r, include=TRUE}

xst=x^2
aux <- data.frame(x, y, xst)

ggplot(aux, aes(x = x, y = y)) + 
  geom_point() + 
geom_smooth(method=lm, formula = y ~ x + I(x^2) , colour="red")

```


<p>&nbsp;</p>


## Significatividad estadistica 

###- (pág. 255-256)

En esta última sección recreamos la técnica del *boostrap* o reseampleo para obtener errores estándares e intervalos de confianza para las estimaciones del FGT. La versión más simple del bootstrap requiere (i) tomar una muestra de tamaño N (el tamaño muestral) de la muestra original con reemplazo, (ii) computar el índice de pobreza deseado y (iii) repetir el procedimiento B veces, con B grande. Esto es lo que haremos mediante un bucle, fijaremos una cantidad de repeticiones, en las que en cada una estaremos tomando un resampleo de la muestra original de igual tamaño. De esta forma la muestra irá cambiando en su composición aleatoriamente y por tanto permitirá generar distintos valores del FGT en cada iteración, a partir de una misma base. El comando en R `sample_n()` realiza esta tarea de resampleo, seteando el dataframe, el tamaño de la nueva muestra y la opción con reposición. La expresión `nrow(df)` indica que la nueva muestra tendrá el mismo tamaño que la base original. Cada valor del indicador se almacena en el objeto *"store"*, sobre el que posteriormente se calcula el desvío, la media y el tamaño. Estos son inputs necesarios para la fórmula que estima los intervalos de confianza de nuestras estimaciones. En este caso estimamos un intervalo de confianza del 95%.


```{r, include=TRUE}

per06 <- read.dta(data_dir %+% "Per/2006/bases/per06_cedlas.dta") %>% 
        filter(cohh==1, !is.na(ipcf))

df <- per06

#genero un resampleo del data frame en cada iteración y para ese data frame obtengo el fgt
store <- c()
rep=50

lp = 128.136

for (i in 1:rep) {
  
   df_sample <- sample_n(df, size=nrow(df), replace=T)
   
   fgt = FGT(df=df_sample, varname=ipcf, var_pondera=pondera, alfa=0, zeta=lp, quiet = TRUE)
   store[i] = fgt
   
}

```


```{r, include=TRUE}
store

sd=sd(store)
mean=mean(store)
n=length(store)

#con intervalo de confianza del 95%
error <- qt(0.975,df=n-1)*sd/sqrt(n)
left <- mean - error; left
right <- mean + error; right


```

A modo de extensión es fácil escribir una función que, tomando los resultados alojados en un vector y el intervalo de confianza deseado, nos devuelve directamente el cálculo del intervalo.

```{r, include=TRUE}

#también es posible hacer un función para calcular los intervalos de confianza
ci <- function(vector, intervalo){
  
  sd=sd(vector)
  mean=mean(vector)
  n= length(vector)
  
  error <- qt((intervalo+1)/2, df=n-1) * sd/sqrt(n)
  result <- c("lower" = mean - error, "upper" = mean + error)
  return(result)
}
```

```{r, include=TRUE}

ci(store, 0.90)
ci(store, 0.95)
ci(store, 0.99)

```




<!--chapter:end:04-Cap4_html.Rmd-->


# Capítulo 5

## Pobreza: extensiones {.unlisted .unnumbered}

#### Escrito por: Cristian Bonavida {-}
#### Last Update: 02/7/2021 {-}
<p>&nbsp;</p>

*Códigos escritos en base a los apéndices del libro "Pobreza y Desigualdad en América Latina" de Gasparini, Cicowiez y Sosa Escudero. El objeto de este material es reproducir la rutina de códigos para STATA presentada en el libro al lenguaje R. Este material es solo de caracter complementario a las explicaciones y detalles conceptuales que se presentan en el libro de texto y los apéndices* 

<p>&nbsp;</p>

## Set Inicial {-}

Cargo las librerias, limpio enviroment, defino el path y atajo para funcion paste

```{r, message=FALSE}

library(dplyr)
library(tidyverse) # Data wrangling
library(tidygraph)
library(readxl)
library(ggplot2)
library(foreign)
library(TAM)
library(margins)

rm(list=ls())    #empiezo limpiando todo 

"%+%" <- function(x,y) paste(x,y,sep = "")      # defino un shorcut parar concat de texto
data_dir <- "C:/Users/HP/Desktop/CEDLAS - UNLP/Apendices en R/Material libro/encuestas-cedlas/Encuestas/"  #seteo directorio 

```

<p>&nbsp;</p>


## Pobreza Multidimensional 

###- (pág. 334-335)

En este primer apartado se muestra cómo puede replicarse el cuadro 5.1 del libro, sobre tasas de pobreza multidimensional en Nicaragua, Perú y Uruguay. Comenzamos cargando y defiendo la base a utilizar

```{r}
nic05 <- read.dta(data_dir %+% "Nic/2005/bases/nic05_cedlas.dta")      
df <- nic05
```

Nos asegurarnos de convertir los missings que puede contener la variable pondera a 0. De esta manera al calcular estimadores ponderados los valores para estas observaciones no tienen ningun peso y evitamos que los missings afecten el nuestros cálculos. 

```{r}
df$pondera <- ifelse(is.na(df$pondera), 0, df$pondera)
```


El bloque de código siguiente asigna a todos los miembros del hogar las variables que solo están definidas para el jefe de hogar. Como  se trata de variables relacionadas con las características de la vivienda, típicamente se encuentran en las bases de datos de hogares y no de personas.

En STATA la solución se propone con un bucle. En este caso aplicamos la misma lógica pero nos valemos de la función `across` que nos permite realizar un mismo cálculo para un conjunto de columnas especificadas. Previamente debemos ordenar y agrupar las observaciones por id.

```{r, message=FALSE}
df <- df %>% arrange(id) %>% group_by(id) %>%
      mutate(
        across(
         .cols  = c(habita, matpreca, agua, banio), #sobre qué columnas aplicar operación
         .fns   = mean,   #que operación/función queremos realizar
          na.rm = TRUE,                                 
         .names = "{col}" #como deben llamarse las nuevas variables
                )
                  )

```

La expresión `names = "{col}"` indica que el nombre de las nuevas columnas sea el nombre de las variables orginales, por lo que las estamos sobreescribiendo.

En las siguiente bloque de código calculamos los indicadores de pobreza para cada una de las dimensiones relevantes, creando un nueva variable en el data frame para cada caso. Empleamos el comando `ifelse` cuando el cálculo es directo y `mutate` cuando se requiere de una variable auxiliar previa.

```{r, message=FALSE}

# (1) ipcf < 2.5 USD
df$indic1 <- ifelse(df$ipcf < 564.12, 1 ,0)

# (2) mas de 3 miembros por cuarto
df <- df %>% mutate(
              rat_miembros_cuartos = miembros/habita,
              aux = case_when(
                    (rat_miembros_cuartos>3 & !is.na(rat_miembros_cuartos)) ~ 1, 
                    (rat_miembros_cuartos<=3 & !is.na(rat_miembros_cuartos)) ~ 0)) %>%
             group_by(id) %>% mutate(indic2=max(aux)) %>% select(-aux)

# (3) vivienda construida con material precario
df$indic3 <- ifelse(df$matpreca==1, 1 ,0)

# (4) vivienda sin acceso a agua potable
df$indic4 <- ifelse(df$agua==0, 1 ,0)

# (5) vivienda sin acceso a baño sanitario 
df$indic5 <- ifelse(df$banio==0, 1 ,0)

# (6) educación promedio menor a 7 años solo para el jefe y conyuge
df <- df %>% mutate(aedu_avg = ifelse(jefe==1 | conyuge==1, mean(aedu, na.rm=TRUE), NA),
                    aux = ifelse(aedu_avg<7 & jefe==1, 1 ,0)) %>% group_by(id) %>%
             mutate(indic6 = max(aux))  %>% select(-aux)

```

Seguidamente obtenemos el porcentaje de personas con privaciones para cada indicador, utilizando la media ponderada.

```{r, message=FALSE, , include = TRUE}

weighted.mean(df$indic1, df$pondera, na.rm=TRUE)*100
weighted.mean(df$indic2, df$pondera, na.rm=TRUE)*100
weighted.mean(df$indic3, df$pondera, na.rm=TRUE)*100
weighted.mean(df$indic4, df$pondera, na.rm=TRUE)*100
weighted.mean(df$indic5, df$pondera, na.rm=TRUE)*100
weighted.mean(df$indic6, df$pondera, na.rm=TRUE)*100

```


La variable "npriv" contiene el número de privaciones de cada individuo. Para crearla utilizamos una operación a nivel de fila con el comando `rowSums` que, combinado con `across`, nos permite sumar todas las columnas especificadas. En este caso especificamos todas las columnas que comienzan con el patron "indic". Notese que a diferencia del uso anterior aqui no se realiza una misma operación repetida para cada columna sino que se especifican las columnas que se incluyen como argumento de la operación suma. 

```{r}

#contar condiciones:por fila sumo todas las columnas que comienzan con indic
df <- df %>% mutate(npriv = rowSums(across(starts_with("indic")))) 

```

Una forma alternativa más intuitiva sería especificar manualmente las columnas a sumar, pero se vuelve poco efeciente en el caso de que estas sean numerosas, por lo que la posibilidad de identificar columnas por patrones se vuelve particularmente atractiva.

```{r}
#manera alternativa
df$npriv = df$indic1 + df$indic2 + df$indic3 + df$indic4 + df$indic5 + df$indic6   

```

A partir de la variable "npriv" se generan las variables "pobre1" a "pobre6" que valen 1 de acuerdo con la cantidad de privaciones que sufre cada individuo. Por ejemplo, la variable "pobre4" vale 1 para los individuos que tienen 4 o más privaciones, y 0 en caso contrario. 

En cada iteración se concatena el prefijo *pobre* con el contador `i`, dandole nombre a cada nueva columna del data frame. Luego se calcula el porcentaje como la media ponderada de esta columna, se redondea e imprime el resultado.

```{r, message=FALSE, , include = TRUE}

#condición de pobreza segun cantidad de privaciones
for (i in 1:6){
  
  df[paste("pobre",i,sep="")] <- ifelse(df$npriv>=i, 1, 0)
  
  p = weighted.mean(df[paste("pobre",i,sep="")], df["pondera"], na.rm=TRUE)*100
  print(paste(i, " privaciones = ", round(p, d=2), "%", sep = ""))
  
}
```

<p>&nbsp;</p>


## Indice Bourguignon y Chakravarty (BC) - Pobreza Multidimensional  

###- (pág. 335-336)

El código a continuación permite reproducir el cuadro 5.2 del texto sobre pobreza multidimensional computada con el índice de Bourguignon y Chakravarty (BC). El cómputo de dicho índice se realiza empleando solo las observaciones que tienen información para las tres dimensiones consideradas en el texto; por lo que se eliminan las observaciones con missing en al menos una de esas dimensiones. Se eligen los valores para los parametros relevantes y se fija el número de dimensiones a considerar

```{r}
df <- df %>% filter(!is.na(ipcf), !is.na(aedu_avg), !is.na(rat_miembros_cuartos))

theta=1  
alpha=2

dim_t=3    #total dimensiones
```

Posteriormente almacenamos los valores de las observacion en una lista, donde cada elemento contiene todos los valores de cada una de las 3 variables a considerar. En vectores separados almacenamos los umbrales y los pesos

```{r}

dimension <- list( df$ipcf,                  # (1) ipcf 
             1/df$rat_miembros_cuartos,      # (2) ratio de miembros por cuarto
             df$aedu_avg                     # (3) educación promedio de jefe y conyuge
              )

umbral <- c(564.119195, 1/3, 7)   #valores para los umbrales de cada dimensiones
wt <- c(1, 1, 1)                  #wt correspondiente
```


El objeto brechas se define como vacío y cada uno de sus elementos se genera en las iteraciones sucesivas del bucle al comparar cada valor de la variable contra los umbrales fijados. El objeto "suma_brechas" se crea como un vector único con valores 0, y luego se reemplaza iterativamente para computar la suma de brechas. El bucle itera *n* veces en total, siendo *n* la cantidad de dimensiones relevadas. En cada iteracion replica la formula de BC para cada dimensión.

```{r}
#defino la lista brechas como vacia para generar cada uno de sus elementos en el bucle
brecha <- list()

suma_brechas <- c(rep(0, nrow(df)))

for (i in 1:dim_t) {
  
  #generar brechas a partir de valores de las dimensiones vs umbrales
  brecha[[i]] <- ifelse(dimension[[i]]<umbral[i], 1-dimension[[i]]/umbral[i], 0)
  
  #construir brechas ponderadas
  brecha[[i]] <- wt[i]/dim_t * (brecha[[i]]^theta)
  
  #computar suma de las brechas. Suma_brechas será = 0 solo si todas las brechas son 0
  suma_brechas = suma_brechas + brecha[[i]]
  
}

```

Finalmente se calcula, para cada individuo, la suma de las brechas ponderadas elevadas a la potencia *theta*, siempre que la suma de las brechas sea distinta de cero. Por último, se computa el índice BC como el cociente entre la suma ponderada de las brechas individuales almacenadas en la variable suma_brechas y la población de referencia.

```{r, include=TRUE}

suma_brechas = ifelse(suma_brechas!=0, suma_brechas^(alpha/theta), suma_brechas)
                      
BC = round(sum(suma_brechas*df$pondera)/sum(df$pondera), d=3)

print(paste("BC =", BC))
```

<p>&nbsp;</p>


## Indice Alkire y Foster (AF) - Pobreza Multidimensional 

###- (pág. 337-338)

A continuación se replica la formula de Alkire y Foster que permite replicar el cuadro 5.3 del texto. Las primeras lineas son identicas al caso anterior, cambiando los parametros de interés y agregando la lista "pobre" como objeto vacío.

```{r}

k=2
alpha=2

dim_t=3

dim <- list( df$ipcf,                       
             1/df$rat_miembros_cuartos,      
             df$aedu_avg                     
)

umbral <- c(564.119195, 1/3, 7)
wt <- c(1, 1, 1)

#defino la lista "brechas" Y pobre como vacia para generar cada uno de sus elementos en el bucle
brecha <- list()
pobre <- list()

```

Nuevamente el bucle itera sobre las *n* dimensiones fijada generando los valores de brecha para cada observación y ahora también completando el objeto binario "pobre" según el valor que toma la brecha. Al finalizar el bucle se construye el objeto "npriv" que contiene el número de dimensiones en que cada individuo fue identificado como pobre (vale cero para los individuos no pobres). El objeto "pobre_k" vale 1 para los individuos que son pobres en, al menos, k dimensiones.

```{r}
for (i in 1:dim_t) {
  
    #generar brechas a partir de valores de las dimensiones vs umbrales
    brecha[[i]] <- ifelse(dim[[i]]<umbral[i], (1-dim[[i]]/umbral[i])^alpha, 0)
    
    #identificar si es pobre en dimensión i
    pobre[[i]] <- ifelse(brecha[[i]]!=0, 1, 0)
  
  }


#identificar pobres en al menos k dimensiones
npriv = pobre[[1]] + pobre[[2]] + pobre[[3]]
pobre_k = ifelse(npriv>=k, 1, 0)
```

La línea siguiente genera la variable "suma_brechas" que, como antes, se emplea luego para almacenar la suma de las brechas en cada una de las dimensiones consideradas. El bucle constuye para cada dimensión los objetos necesarios para replicar la formula de AK

```{r}
for (i in 1:dim_t) {  
  
    #brechas positivas solo si el número de privaciones mayor a k
    brecha[[i]] <- ifelse(pobre_k!=1, 0, brecha[[i]])
    
    #construir brechas ponderadas
    brecha[[i]] <- wt[i] * brecha[[i]]
    
    #computar suma de las brechas. Suma_brechas será = 0 solo si todas las brechas son 0
    suma_brechas = suma_brechas + brecha[[i]]
    
}

```

Por último se computa y se redondean los valores del índice de AF

```{r}
AK = round(sum(suma_brechas*df$pondera)/(dim_t*sum(df$pondera)), d=5)

print(paste("AK =", AK))
```

<p>&nbsp;</p>


## Perfiles de Pobreza 

###- (pág. 338-339)

El bloque de código a continuación puede emplearse para computar el perfil de pobreza monetaria para vivienda y servicios que se muestra en el cuadro 5.8. El código del ejemplo se aplica a la EPH (Encuesta Permanente de Hogares) de Paraguay para el año 2007. Luego de cargar y definir la base, la tercer linea de código genera la variable "hh" que vale 1 para una única observación de cada hogar, a partir de indentificar las observaciones duplicadas. El comando `duplicated` asigna valor `FALSE` a la primera observación del por hogar, y `TRUE` a todo el resto.

```{r}
pry07 <- read.dta(data_dir %+% "Par/2007/bases/par07_cedlas.dta")
df <- pry07

df$hh <- ifelse(duplicated(df$id)==FALSE, 1,0)
```


Luego generamos en el data frame la variable indicativa de pobreza monetaria y a partir de ella computamos para el grupo de pobres y no pobres, el promedio de las variables "habita", "matpreca", "agua", "banio" y "elect", indicado la proporción de personas que cuenta con estos servicios. 

```{r, include=TRUE}
df$pobre <- ifelse(df$ipcf<205970.366, 1, 0)

df %>% filter(hh==1 & !is.na(pobre)) %>% group_by(pobre) %>%    
       summarise( mean_habita = weighted.mean(habita, pondera, na.rm = TRUE),
                  mean_matpreca = weighted.mean(matpreca,pondera, na.rm = TRUE),
                  mean_banio = weighted.mean(banio, pondera, na.rm = TRUE),
                  mean_agua = weighted.mean(agua, pondera, na.rm = TRUE),
                  mean_elect = weighted.mean(elect, pondera, na.rm = TRUE))
```

Con el comando `ttest` buscamos evaluar la significatividad estadística de estas diferencias de medias entre pobres y no pobres para las variables incluidas. Para esto elegimos el nivel de confianza y definimos un bucle que itera sobre cada variable respectiva del data frame. Dentro de él construimos un objeto "x" que contiene los valores para estas variables sólo para una única observación por hogar y para el grupo de pobres y otro objeto "y" con los mismos datos para el grupo de no pobres. A partir de estos objetos se evaluan las significatividad de la diferencia de medias entre ambos grupos y se reporta si su p-valor es mayor al nivel de confianza fijado.


```{r, include=TRUE}

set_confidence = 95 
confidence = 1 - set_confidence/100

dim <- c("habita", "matpreca", "banio", "agua", "elect")
for (i in dim){
  
  print(i)
  x <- df[df$pobre==1 & df$hh==1, colnames(df)==i]
  y <- df[df$pobre==0 & df$hh==1, colnames(df)==i]
  
  test = t.test(x,y)
  print(paste("No significative mean diff:", test$p.value > confidence))

  }

```

<p>&nbsp;</p>


## Perfiles de Pobreza Condicionados 

###- (pág. 340-341)

El bloque de código siguiente permite replicar el cuadro 5.12, que muestra perfiles condicionados de pobreza. En el ejemplo se emplea la encuesta de México para el año 2006.
Luego de cargar la base eliminamos las observaciones incohrentes y al igual que antes generamos la variable indicativa de pobreza monetaria línea de 2.5 dólares. Las lineas siguientes agregan nuevas variables al data frame que suman la cantidad de individuos que pertenecen a distintos grupos etarios dentro de cada hogar, calculan el ratio de miembros por habitaciones y los valores de educación y edad al cuadrado

```{r}
#cargo base 
mex06 <- read.dta(data_dir %+% "Mex/2006/bases/mex06_cedlas.dta")      
df <- mex06

df <- df %>% filter(cohh==1)
df$pobre <- ifelse(df$ipcf<608.24533, 1, 0)

#número de miembros en cada grupo
df <- df %>% arrange(id) %>% group_by(id) %>%
             mutate( miembros_edad_0015 = sum(ifelse(edad<=15, 1, 0)),
                     miembros_edad_1625 = sum(ifelse(edad %in% (16:25), 1, 0)),
                     miembros_edad_2640 = sum(ifelse(edad %in% (26:40), 1, 0)),
                     miembros_edad_4160 = sum(ifelse(edad %in% (41:64), 1, 0)),
                     miembros_edad_65mas= sum(ifelse(edad>=65, 1, 0)),
                     
                     rat_miembros_cuartos = miembros/habita,
                     
                     aedu2=aedu^2,
                     edad2=edad^2 )
```

Las líneas siguientes contienen la sentencia que estima, para los jefes de hogar, el modelo probit para la probabilidad de ser pobre. Para ello empleamos el comando `glm` en el cual definimos la variable independiente y todo el conjunto de regresores, indicamos el data frame referido y la familia de modelos que buscamos estimar. Esta estimación la guardamos en el objeto "probit" que luego visualizamos con un `summary`

```{r, include=FALSE}
#el probit solo para jefes de hogar                   
df_jefes <-df[df$jefe== 1,] 

probit <- glm(pobre ~ aedu + aedu2 + edad + edad2 + miembros_edad_0015 + miembros_edad_1625 + miembros_edad_2640 +
                      miembros_edad_4160 + miembros_edad_65mas + hombre + casado + desocupa + urbano + agua + banio +
                      matpreca + cloacas + rat_miembros_cuartos + perii, 
              
             data = df_jefes, family = binomial(link = "probit"))

#visualizo resultados
summary(probit)
```

Para computar los efectos marginales para el rango 0 a 22 años de educación del jefe de hogar, empleamos el comando `margins`. Para ello indicamos donde almacenamos nuestra estimación (objeto probit), para qué variables deseamos calcular los efectos (aedu) y sobre qué valores evaluarlos (0:22). 

```{r, include=TRUE}
store <- summary(margins(probit, variables = "aedu", at = list(aedu = 0:22)))
store
```

Esta información la guardamos en un objeto llamado "store", del cual nos interesa recuperar el valor de los coficientes almacenados bajo el nombre AME (Average Mean Effect). Con ellos generamos un vector "y" que denota los efectos para cada valor de años de educación, los cuale guardamos en el vector "x". Finalmente graficamos la relación.

```{r, include=TRUE}
y = store$AME
x = seq(0:22)
  
plot(x, y,
     ylab = "Efecto Marginal",
     xlab = "Años Educación")
```

Una forma más directa de graficar los efectos marginales es a partir del comando `cplot` de la familia margins, que estima automaticamente estos mismos valores a partir de la estimación del modelo probit

```{r}
#cplot(probit, "aedu", what = "effect", main = "Average Marginal Effect of Weight")
```


<!--chapter:end:05-Cap5_html.Rmd-->

